---
title: "Exam 1"
author: "Denis Ostroushko"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    extra_dependencies: ["float"]
editor_options: 
  markdown: 
    wrap: 72
---



```{r, echo = F}
knitr::opts_chunk$set(echo = F, message = F, warning = F, fig.pos = "!H", fig.height=4, fig.width=7, fig.align='center')
options(scipen=999)
```

```{r load all packages from the master file , include=F}
source('/Users/denisostroushko/Desktop/UofM MS/MS Fall 2022/Puhb 7405/Package master list .R')
```

# Problem 1

```{r load data problem 1 }
prob_1 <- read_xlsx('/Users/denisostroushko/Desktop/UofM MS/MS Fall 2022/Puhb 7405/Data Sets/Exam 1/Exam 1 Emergency-Service-E-22.xlsx')

colnames(prob_1) <- c("n_visits", "complaint", "residency", "gender", "revenue", "hours")

#  str(prob_1)
#  unique(prob_1$residency)
#  unique(prob_1$gender)

prob_1$complaint_rate_1000 <- with(prob_1, complaint / n_visits * 1000)

```

## 1- A

Before fitting the model I like to explore the distribution shape of the response variable and collect some fundamental 
summary statistics. Knowing the shape and the spread of the response variable will help us manage the expectation regarding model fit 
and variance of residuals. 

```{r complain rate plot}

ggplot(data = prob_1, 
       aes(x = complaint_rate_1000)) + 
  geom_histogram(binwidth = .25, color = "black", fill = "light yellow") + 
  
  geom_vline(aes(colour = "Average Complaint Rate", xintercept = mean(complaint_rate_1000)), size = 1) + 
  geom_vline(aes(colour = "Median Complaint Rate", xintercept = median(complaint_rate_1000)), size = 1) + 
  scale_color_manual(values = c("Average Complaint Rate" = "blue", "Median Complaint Rate" = "red")) + 
  
  xlab("Complaint Rates Per 1,000") + 
  ylab("Count") + 
  ggtitle(paste("Disbtribution of Complaint Rates per 1,000 Visits", 
                "\n Average: ", round(mean(prob_1$complaint_rate_1000), 2), 
                "\n Median: ", round(median(prob_1$complaint_rate_1000), 2)))  + 
  
  theme(legend.position = "bottom") + 
  guides(color = guide_legend(nrow=2, byrow=TRUE)) + 
  theme_minimal()

```

```{R, eval  =F}

```

The distribution of complaints per 1,000 visits somewhat balanced without extreme outliers. The mean is pretty close to the 
median, suggesting again that more extreme values on the upper end of complaints per 1,000 do not knew the mean very much. 

The two tables below describe the distribution of numeric variables in the data set, as well as correlation between the three of them.

We can see that the scales of predictors and complaints per 1,000 vary greatly, so we should expect that the coefficients are 
going to be very small, probably in the $0.001$ to $0.0001$ range. 

```{r}

s_df <- 
  data.frame(
    sapply(prob_1 %>% select(complaint_rate_1000, revenue, hours), min),
    sapply(prob_1 %>% select(complaint_rate_1000, revenue, hours), max),
    sapply(prob_1 %>% select(complaint_rate_1000, revenue, hours), mean),
    sapply(prob_1 %>% select(complaint_rate_1000, revenue, hours), sd)
  )


colnames(s_df) <- c("Min", "Max", "Mean", "S.D")


s_df$Variables <- rownames(s_df)
rownames(s_df) <- NULL

round_2 <- function(x){round(x,2)}
s_df[, 1:(length(s_df)-1)] <- lapply(s_df[, 1:(length(s_df)-1)], round_2)


s_df <- s_df %>% select(Variables, everything())

s_df %>% 
  kbl(caption = "Summary of Numeric Variables", 
      booktabs = T) %>% 
  kable_styling(latex_options = c("HOLD_position", "striped"))
```

```{r}

prob_1_model_data <- prob_1 %>% select(complaint_rate_1000, residency , gender, revenue, hours)

cor_m <- data.frame(cor(prob_1_model_data %>% select(-gender, -residency)))

rownames(cor_m) <-  c("Complaint Rate per 1,000", "Revenue", "Hours Worked")

cor_m %>% 
  kbl(col.names = c("Complaint Rate per 1,000", "Revenue", "Hours Worked"), 
      caption = "Correaltion of Numeric Covariates", 
      booktabs = T)

```

We continue to perform explanatory data analysis in this section by looking at the scatter plots of predictors versus complaint 
rates. It does not appear that revenue is related to complaint rate at all. 

```{r}
  ggplot(data = prob_1_model_data, 
         aes(x = revenue, 
             y = complaint_rate_1000)) + 
    
    geom_point() + 
    
    geom_smooth(aes(color = "Smooth Trend Line")) + 
    geom_smooth(method = "lm", se = F, aes(color = "Fitted Regression Line")) + 
    scale_color_manual(values = c("Smooth Trend Line" = "blue", "Fitted Regression Line" = "red")) + 
    
    theme(legend.position = "bottom") + 
    guides(color = guide_legend(nrow=2, byrow=TRUE)) + 
    
    theme_minimal() + 
    xlab("Revenue") + 
    ylab("Complaints Per 1000") + 
    labs(color = "Line Type") + 
    ggtitle("Relationship between Revenue and Complaint Rate")
```

We can see that the number of hours worked is somewhat linearly related to the complaint rates, suggesting the practitioners
who work more hours tend to accumulate higher complaint rate, however, the the variance of values is very large around the 
suggested regression line, so we might not be able to detect a statistically significant relationship when fitting the model. 

```{r}
  ggplot(data = prob_1_model_data, 
         aes(x = hours, 
             y = complaint_rate_1000)) + 
    
    geom_point() + 
    
    geom_smooth(aes(color = "Smooth Trend Line")) + 
    geom_smooth(method = "lm", se = F, aes(color = "Fitted Regression Line")) + 
    scale_color_manual(values = c("Smooth Trend Line" = "blue", "Fitted Regression Line" = "red")) + 
    
    theme(legend.position = "bottom") + 
    guides(color = guide_legend(nrow=2, byrow=TRUE)) + 
    
    theme_minimal() + 
    xlab("Hours Worked") + 
    ylab("Complaints Per 1000") +  
    labs(color = "Line Type") + 
    ggtitle("Relationship between Hours Worked and Complaint Rate")
```

Overall, both plots suggest that linear fit is appropriate for both of these variables. Smooth Loess function does not show 
any consistent curvature in the data, but rather randomly fluctuates around the fitted regression line. 

We have categorical predictors also: 

Residency has two levels: `r paste(unique(prob_1$residency) )` with `r paste0(round(table(prob_1$residency)/nrow(prob_1),4)*100, "%")` class presence respectively. It does not appear that the median and mean values are different across the two residency levels. 

```{r}
ggplot(data = prob_1_model_data, 
       aes(x = residency, 
           y = complaint_rate_1000)) + 
    
    geom_point() + 
    
    geom_boxplot()+ 
    
    theme_minimal() + 
    xlab("Recidency") + 
    ylab("Complaints Per 1000") +  
    labs(color = "Line Type") + 
    ggtitle("Relationship Residency Status and Complaint Rate")
```

Gender has two levels: `r paste(unique(prob_1$gender) )` with `r paste0(round(table(prob_1$gender)/nrow(prob_1),4)*100, "%")` class presence respectively. It does not appear that the median and mean values are different across the two gender levels. 

```{r }

ggplot(data = prob_1_model_data, 
       aes(x = gender, 
           y = complaint_rate_1000)) + 
    
    geom_point() + 
    
    geom_boxplot()+ 
    
    theme_minimal() + 
    xlab("Gender") + 
    ylab("Complaints Per 1000") +  
    labs(color = "Line Type") + 
    ggtitle("Relationship Gender Status and Complaint Rate")

```

Now we are ready to fit and examine the Normal Error Regression Model. When fitting any kind of a model, we need to be careful with 
the assumptions we take on. **Model Assumptions**  are listed below

1. Residuals, Error Terms, are normally distributed with mean $\mu = 0$ and constant variance $\sigma^2$. 

2. Since fitted values depend on model parameters $\hat \beta_i$ and errors $e_i$ , we assume each outcome $Y_i$ comes from a normal distribution with mean $\mu = E[Y_i]$ and variance $\sigma^2$. 

3. We assume that variance of residuals is constant. 

4. Errors are independent and each unit of interest, a data point, is also independent of other observations in the sample. 

5. The model is linear because $\hat Y_i$ can be expressed as a linear combination of weights, coefficients, $\hat \beta_i$ and 
constant observed data points $X_i$. 

6. Predictors are not correlated or weakly correlated. 

After listing model assumptions, we can state the mode: 

$$\Large E[Complaint \ Rate] = \hat \beta_0 + \hat \beta_1 * X_1 + \hat \beta_2 * X_2 + \hat \beta_3 * X_3 + \hat \beta_4 * X_4 = $$

$$\Large E[Complaint \ Rate] = \hat \beta_0 + \hat \beta_1 * Revenue + \hat \beta_2 * Hours \ Worked  + \hat \beta_3 *Gender  + \hat \beta_4 * Residency $$

**Overall ANOVA** 

Before investigating individual coefficients and t-test for predictors, we want to look at the overall ANOVA table, and overall 
F-test. We want to see if the set of all predictors is helpful at explaining the variance of complaint rates per 1,000, and therefore
we will know if some of all coefficients are statistically different from 0. 

ANOVA table for the F-test is given below: 

```{r}

full <- lm(complaint_rate_1000 ~ revenue + hours + gender + residency, data = prob_1_model_data)

SSR <- sum(
  (full$fitted.values - mean(prob_1_model_data$complaint_rate_1000))^2
)

SSE <- sum(full$residuals^2)
SSTO <- sum((mean(prob_1_model_data$complaint_rate_1000) - prob_1_model_data$complaint_rate_1000)^2 )
  
df_ssr <- length(prob_1_model_data) - 1
df_sse <- nrow(prob_1_model_data) - length(prob_1_model_data)

res <- 
  data.frame(
    Source = c("Regression", "Error", "Total"), 
    SSR = c(SSR, SSE, SSTO), 
    DF = c(df_ssr, df_sse, nrow(prob_1_model_data)-1)
  )

res$MS <- NA
res[1:2,]$MS <- res[1:2,]$SSR / res[1:2,]$DF

res$`F Statistic` <- NA
res[1,]$`F Statistic` <- round((SSR/df_ssr) / (SSE/df_sse),2)

res$`P(F* > F)` <- NA
res[1,]$`P(F* > F)` <- round(1 - pf((SSR/df_ssr) / (SSE/df_sse), df1 = df_ssr, df2 = df_sse),4)
  
res %>% 
  kbl(booktabs = T, align = 'c') %>% 
  kable_styling(latex_options = c("HOLD_position", "striped"))

```

* Null Hypothesis: $H_0: \beta_1 = \beta_2 = ... = \beta_{p-1}$

* Alternative Hypothesis: $H_a:$ Not all coefficients $\beta_i$ are zero

* $F-$statistic: `r round((SSR/df_ssr) / (SSE/df_sse),2)`

* Cutoff $F^*$-statistic: `r round(qf(1-.05, df1 = df_ssr, df2 = df_sse),4)`

* So, $F < F^*$, therefore we do not have enough evidence to reject the null hypothesis to conclude that some or all 
  coefficients $\beta_i$ are consistently different from zero. 

* Moreover, $P(F^* > F) =$ `r round(1 - pf((SSR/df_ssr) / (SSE/df_sse), df1 = df_ssr, df2 = df_sse),4)`
  
* Conclusion: There is not enough statistical evidence that every predictor has a coefficient different from 0. Therefore, we can't 
reject the null hypothesis. When we look at the individual t tests for coefficients, we might see some suggestive relationships, 
supported by the somewhat big values of the t-statistic and small p-values, but none of them should be statistically significant. 

Table below shows **Regression Coefficients** and model summary. Like we expected, these coefficients are small because the scale and 
range of predictors and response variable are not the same. 

```{r}
res_reg <- data.frame(summary(full)$coefficients)
res_reg$var <- rownames(res_reg)
rownames(res_reg) <- NULL
res_reg <- res_reg %>% select(var, everything())
res_reg <-
  res_reg %>% mutate_at(vars(Estimate, `Std..Error`, t.value, `Pr...t..`),
                                 funs(round(., 6)
                                      )
                                 )

colnames(res_reg) <- c("Predictor", "Estiamte", "Standard Error", "T Value", "P value")
res_reg %>%
  kbl(booktabs = T, align = c('l','c', 'c', 'c', 'c')) %>%
  kable_styling(latex_options = c("striped", "HOLD_position"))

rev_c  <- res_reg[res_reg$Predictor == "revenue", ]$`Estiamte`
h_c  <- res_reg[res_reg$Predictor == "hours", ]$`Estiamte`
m_c  <- res_reg[res_reg$Predictor == "genderM", ]$`Estiamte`
r_c  <- res_reg[res_reg$Predictor == "residencyY", ]$`Estiamte`

```


* R square and `r round(summary(full)$r.square,4)`

* Adjusted R Square `r round(summary(full)$adj.r.squared,4)`

* Coefficients explanation: 

    + `revenue` is revenue in dollars per hour is a continuous predictor. When revenue increases by 1 dollar per hour, 
      we expect the number of complaints to increase by `r rev_c`, after adjusting for other predictors. 
      
    + `hours` is the number of hours worded, and is a continuous predictor. With each additional hour of work, 
      we expect the number of complaints to increase by `r rev_c`, after adjusting for other predictors. 
    
    + `genderM` is a coefficient for the group of men practitioners, when compared with women practitioners, which is a reference
        level here. Physicians who are men on average are expected to have `r m_c` more complaints per 1,000 when compared with the 
        women physicians after adjusting for other predictors. 
        
    + `residencyY` is a coefficient for the group of practitioners who have a residency fellowship, 
        compared with practitioners who do not participate in such program, which is a reference
        level here. Physicians with fellowship are expected to have `r abs(r_c)` less complaints per 1,000 when compared with the 
        women physicians after adjusting for other predictors. 

* As expected, none of these predictors show any evidence of statistical significance, but the results are not contradictory. 
  Doctors who are trained to work in emergency medicine should be able to do their work better, and therefore should have less 
  complaints. Additional hours may result in extra complaints, if the doctor is overworked and their ability to perform 
  reduces. 

## 1- B

A special interest is to investigate how the extra hours of work impact the average complaint rate for each practitioner, given 
their characteristics we adjust for. 

We begin the inference of this variable with a formal t-test. 

```{r}
tv <- res_reg[res_reg$Predictor == "hours", ]$`T Value`
pv <- res_reg[res_reg$Predictor == "hours", ]$`P value`
```

*   Null Hypothesis: $H_0: \hat \beta_4 = 0$

*   Alternative Hypothesis: $H_a:  \hat \beta_4 \neq 0$$

*   Test statistic $T:$ `r tv`

*   $P(t^* > t) =$ `r pv`

* Conclusion: p-value is above 0.05 so there is not enough statistical evidence to reject the null hypothesis to conclude that the 
  additional hour of work consistently results in the average increase of complaint rates. However, the p-value is not greatly 
  far for the accepted significance level, so, this relationship is suggestive. Perhaps, with mode data, or a better statistical 
  model we will be able to verify that this relationship is in fact consistent. My recommendation to the managers and decision 
  makers would be to pay close attention to this factor, because even though the test shows no significance, the relationship is 
  perhaps still real, and can't be detected from this sample. 

```{r}
se <- res_reg[res_reg$Predictor == "hours", ]$`Standard Error`

ci <- data.frame(confint(full))

ci$name <- rownames(ci)

est_lb <- round(ci[ci$name == "hours", ]$X2.5.., 6)
est_ub <- round(ci[ci$name == "hours", ]$X97.5.., 6)
  
```

One additional Hour worked results in `r h_c` additional complaints on average. 

However, it makes more sense to, say, look at 20 hours. So, an average increase in complaint rates per 1,000 is 
`r h_c` * 20 = `r h_c * 20`. 

It is reasonable to expect that a practitioner who is overworked will have an extra 20 hours of work on top of regular hours in 
one week, especially in a busy or underfunded facility. 


**C.I.** 

Using formula $C.I. \ bounds = Estimate \pm 1.96 * Standard \ Error$

C.I. for the estimate `r h_c` with a `r se` standard error is (`r est_lb`, `r est_ub`)

Similar to the coefficient, we can perform a linear transformation of the lower and upper bounds, and obtain a confidence
interval for the effect of extra 20 hours of work. So, an average increase in complaint rates per 1,000 for the extra 20 hours 
of work is `r h_c` * 20 = `r h_c * 20`, with a confidence interval (`r est_lb * 20`, `r est_ub * 20`). 

It appears that most of the confidence interval is above 0, in fact, `r round(est_ub/(est_ub - est_lb),4)*100 `% of values in 
the confidence interval are above 0. So, even though this evidence is pretty weak, we would still pay attention to this variables 
as a source of Y variance explanation. 

\newpage 

## 1- C

The plot below shows the relationship between fitted values and studentized residuals from the regression model we built and 
evaluated in the previous two sections. There is no linear trend, as evidenced by the flat fitted regression line. 

Smooth trend line suggests that either there is some violation of assumptions at the lower and upper ends of the fitted values, 
or there is simply a small number of values there. 

In any case, variance appears to be somewhat constant, we do not see a megaphone or a violin shape. However, residuals above 
0 tend to have an upper bound of around 2, whereas residuals below 0 tend to have a lower bound of around 1.5. Overall, this is 
not a huge cause for concern, but something we should keep an eye. 

```{r}

resid_plot_df <- 
  data.frame(
    resid = rstandard(full) , 
    fit = full$fitted.values
  )

ggplot(data = resid_plot_df, 
       aes(x = fit, 
           y = resid)) + 
    geom_point() + 
    
    geom_smooth(aes(color = "Smooth Trend Line")) + 
    geom_smooth(method = "lm", se = F, aes(color = "Fitted Regression Line")) + 
    scale_color_manual(values = c("Smooth Trend Line" = "blue", "Fitted Regression Line" = "red")) + 
    
    theme_minimal() + 
    xlab("Fitted Values") + 
    ylab("Residuals") + 
    ggtitle("Relationship between Hours Worked and Complaint Rate") +  
    labs(color = "Line Type")

```

\newpage

## 1- D

In order to evaluate the nature of the relationship between complaint rates per 1,000 and the number of hours worked after adjusting
for the other 3 predictors we use an added variable plot. In order to do that we will need to obtain two sets of residuals from 
the two models: 

* Model 1: obtains residuals for $Y =$ Complaints per 1,000 visits. We denote these residuals as $\epsilon_Y = e(Y|X_1, ..., X_4)$: 

    + $\Large Y = \hat \beta_0 + \hat \beta_1 * Revenue + \hat \beta_2 * Hours \ Worked  + \hat \beta_3 *Gender  + \hat \beta_4 * Residency + \epsilon_{Y}$
    
* Model 2: obtains residuals for $X_2 = $ Number of Hours Worked. We denote these residuals as $\epsilon_X = e(X_2|X_1, X_3, X_4)$: 

    + $\Large X_2 = \hat \beta_0 + \hat \beta_1 * Revenue + \hat \beta_3 *Gender  + \hat \beta_4 * Residency + \epsilon_{x}$
    
Plot below shows the relationship between the two sets of residuals: 

```{r}
y_reg <- lm(complaint_rate_1000 ~ residency + gender + revenue, data = prob_1_model_data)
x_reg <- lm(hours ~ residency + gender + revenue, data = prob_1_model_data)

d <- 
  data.frame(
    y_res = y_reg$residuals,
    x_res = x_reg$residuals, 
    x_2_var = prob_1_model_data$hours
  )

  ggplot(data = d, 
         aes(x = x_res, 
             y = y_res)) + 
    
    geom_point() + 
    
    geom_smooth(aes(color = "Smooth Trend Line")) + 
    geom_smooth(method = "lm", se = F, aes(color = "Fitted Regression Line")) + 
    scale_color_manual(values = c("Smooth Trend Line" = "blue", "Fitted Regression Line" = "red")) + 
    
    theme_minimal() + 
    
    xlab("e[X_2 | X_1, X_3, X_4]") +
    ylab("e[Y | X_1, X_2, X_3, X_4]") +
    
    labs(color = "Line Type") + 
    ggtitle("Added Variable Plot for the Number of Hours Worked")
  
```

This plots gives us two pieces fo evidence that we sue to describe the marginal relationship of $X_2$ and $Y$, after adjusting 
for three other predictors: 

1. The relationship between $X_2$ and $Y$, after accounting for other predictors, is linear in its nature. We can see that the 
    smooth trend line fluctuates randomly around the fitted regression line, suggesting that there is no consistent curved,
    or other non-linear relationship between the number of hours worked and the complaint rate. 
    
2. The fitted regression line that confirm linear relationship has a positive, upward facing, slope, suggesting that the number 
    of hours worked can be used a potentially useful predictor that help increase the percentage of variation of in the 
    complaint rate. This supports our previous conclusion that $X_2$ may be employed as a useful predictor of $Y$, but this 
    model does not gives us enough sufficient evidence to make such a claim. 

```{r, eval = F}
  ggplot(data = d, 
         aes(x = x_2_var, 
             y = y_res)) + 
    
    geom_point() + 
    
    geom_smooth(aes(color = "Smooth Trend Line")) + 
    geom_smooth(method = "lm", se = F, aes(color = "Fitted Regression Line")) + 
    scale_color_manual(values = c("Smooth Trend Line" = "blue", "Fitted Regression Line" = "red")) + 
    
    theme_minimal() + 
    xlab("Hours Worked") + 
    ylab("Y Residuals") +  
    labs(color = "Line Type") + 
    ggtitle("Residulas against Number of Hours Worked")
```

\newpage 

# Problem 2

```{r}
prob_2 <- read_xlsx('/Users/denisostroushko/Desktop/UofM MS/MS Fall 2022/Puhb 7405/Data Sets/Exam 1/Exam 1 Covid-Vaccination-E-22.xlsx')

colnames(prob_2) <- c("county", "delta_deaths", "total_deaths", "size", "v_rate", "region")
# size is expressed in 1,000

```

## 2 - A

In order to pick between the two-sample T-test and Wilcoxon test we need to understand the shape of the distribution, in particular 
the spread of values, and the effect that extreme values and outliers can have on the t-test. While t-test is robust and produces 
that we can rely on, it is known that in heavily skewed distributions non-parametric methods that rely on rank of observations 
will be more effective. On the other hand, if we do not see a heavily skewed distribution, but instead see a distribution that is 
approximately normal, we want to use a t-test, because for such data Wilcoxon has only 95% of statistical power of the the T-test. 

The plot below shows vaccination rates for the metro area counties. It is pretty card to make any conclusions from this plot, and 
this group of observations, since there are only `r nrow(prob_2 %>% filter(region == 1))` counties that make up Metro area. 

```{r}

ggplot(data = prob_2 %>% filter(region == 1), 
       aes(x = v_rate)) + 
  geom_histogram(binwidth = 1, color = "black", fill = "light yellow") + 
  
  geom_vline(aes(colour = "Average Vaccination Rate", xintercept = mean(v_rate)), size = 1) + 
  geom_vline(aes(colour = "Median Vaccination Rate", xintercept = median(v_rate)), size = 1) + 
  scale_color_manual(values = c("Average Vaccination Rate" = "blue", "Median Vaccination Rate" = "red")) + 
  labs(color = "Summary Statistics") + 
  
  xlab("Vaccination Rates") + 
  ylab("Count") + 
  ggtitle(paste("Disbtribution of Vaccination Rates in Metro Area MN Counties", 
                "\n Average: ", round(mean((prob_2 %>% filter(region == 1))$v_rate), 2), 
                "\n Median: ", round(median((prob_2 %>% filter(region == 1))$v_rate), 2))) + 
  theme_minimal()

```

On the other hand, there are `r nrow(prob_2 %>% filter(region == 0))` Outstate counties. These counties produce a balanced 
bell-shaped distribution that looks normal. However, there are a few outliers. Outstate counties with unusually high
vaccination rates are: `r paste(prob_2[prob_2$v_rate > 87 & prob_2$region == 0, ]$county, collapse = " and ")` with 
`r paste(prob_2[prob_2$county %in% c("Olmsted", "Cook"), ]$v_rate, collapse = " and ")` vaccination rates, respectively. 

Olmsted county includes Rochester, a pretty big city by the outstate standards. Moreover, Mayo clinic is located there, so we 
can speculate that more people should have more trusting relationship with medicine and public health there. 

Cook county is located by the Canadian Border, I am not sure what conclusion we can draw from this fact. 

```{r}

ggplot(data = prob_2 %>% filter(region == 0), 
       aes(x = v_rate)) + 
  geom_histogram(binwidth = 1, color = "black", fill = "light yellow") + 
  
  geom_vline(aes(colour = "Average Vaccination Rate", xintercept = mean(v_rate)), size = 1) + 
  geom_vline(aes(colour = "Median Vaccination Rate", xintercept = median(v_rate)), size = 1) + 
  scale_color_manual(values = c("Average Vaccination Rate" = "blue", "Median Vaccination Rate" = "red")) + 
  labs(color = "Summary Statistics") + 
  
  xlab("Vaccination Rates") + 
  ylab("Count") + 
  ggtitle(paste("Disbtribution of Vaccination Rates in Outstate MN Counties", 
                "\n Average: ", round(mean((prob_2 %>% filter(region == 0))$v_rate), 2), 
                "\n Median: ", round(median((prob_2 %>% filter(region == 0))$v_rate), 2))) + 
  theme_minimal()

```

The table below summarized two distributions in terms of most common summary statistics. This table gives me the impression that 
we will be able to conclude that the two sample means are in fact different because the we have small standard deviations, 
while the two means are quite different. 

```{r}
## summarize the data 
prob_2 %>% 
  group_by(region) %>% 
  summarize(
    n = n(), 
    mean = mean(v_rate),
    median = median(v_rate),
    sd = round(sd(v_rate),2)
  ) %>% 
  mutate(Reg = ifelse(region == 1, "Metro", "Outstate")) %>% 
  select(-region) %>% 
  select(Reg, everything()) %>% 
## pipe it into kable right away
    kbl(booktabs = T, 
        caption = "Vaccination Rates Summary by County Type", 
        col.names = c("Type", "N", "Mean", "Median", "S.D.")) %>%
      kable_styling(latex_options = c("HOLD_position", "striped"))

```

Before conducting the test, we also wish to see if the overall distribution of the two samples combined is normal. Recall, there 
are only `r nrow(prob_2 %>% filter(region == 1))` counties in the metro area, so we should combine the two samples for this 
verification. 

We can test the Normality of Vaccination Rates distribution against the expected quantiles of standard normal distribution. 

We can calculate these expected values using the formula:
$$\sqrt {Variance} \times z(\frac{Value - .375}{N + .25}) $$

It appears that the sample of data we have is approximately normally distributed. 

```{r}

mse <- sd(prob_2$v_rate)

prob_2_n <- prob_2 %>% arrange(v_rate)

prob_2_n$resid_rank <- as.numeric(rownames(prob_2_n))

N <- nrow(prob_2_n)

prob_2_n$expected_v_rate <- sqrt(mse) * qnorm((prob_2_n$resid_rank - .375)/(N + .25))

corr <- cor(prob_2_n$v_rate, prob_2_n$expected_v_rate)

ggplot(data= prob_2_n, 
       aes(x = expected_v_rate, y = v_rate)) + geom_point() + 
  geom_smooth(method = "lm", color = "red") + 
  ylab("Vacciantion Rate") + 
  xlab("Expected Vacciantion Rate") + 
  ggtitle(paste("Correlation between Observed and Expected", round(cor(prob_2_n$v_rate, prob_2_n$expected_v_rate),3)))+ 
  theme_minimal()

```

Therefore, we will use T test here. 
  
```{r}

wil_res <- 
  wilcox.test( 
            y = prob_2[prob_2$region == 1, ]$v_rate, 
            x = prob_2[prob_2$region == 0, ]$v_rate,
            
            conf.int = T,
            conf.level = .95, 
            est_diff = T)

t_test <- 
  t.test(y = prob_2[prob_2$region == 1, ]$v_rate, 
            x = prob_2[prob_2$region == 0, ]$v_rate,
            
            conf.int = T,
            conf.level = .95, 
            est_diff = T)

med_1 <- mean((prob_2 %>% filter(region == 1))$v_rate)
med_0 <- mean((prob_2 %>% filter(region == 0))$v_rate)


```

Test results summary and interpretation are given below:

*   Null Hypothesis: $H_0: \mu_{metro \ area} = \mu_{outstate}$ 

*   Test statistic: $H_a: \mu_{metro \ area} \neq \mu_{outstate}$

*   Metro area mean vaccination rate is `r med_1`, while outstate median vaccination mean is `r med_0`

*   Estimated difference is `r med_0-med_1 `, bounded by (`r paste(round(t_test$conf.int[1], 4), ",", round(t_test$conf.int[2], 4))`)

*   Test statistic $T$: `r t_test$statistic`

*   $P(T^* > T) =$ `r round(t_test$p.value,6)`

* Conclusion: P-value is small, so we can reject the null hypothesis and conclude that the average difference in vaccination rates 
  on the county level is statistically significant between metro and rural areas. On average, we can expect metro area counties 
  to have `r abs(med_0-med_1)` more vaccines administered per 1,000 county residents. 
    
## 2 - B

```{r}
prob_2$death_rate <- with(prob_2, delta_deaths / size)
```

Once again, we begin the problem with the distribution visualization. 
Due to a small number of observations in the sample of metro counties we have a histogram that does not really provide much 
information. 

```{r}

ggplot(data = prob_2 %>% filter(region == 1), 
       aes(x = death_rate)) + 
  geom_histogram(binwidth = .1, color = "black", fill = "light yellow") + 
  
  geom_vline(aes(colour = "Average Delta Death Rate", xintercept = mean(death_rate)), size = 1) + 
  geom_vline(aes(colour = "Median Delta Death Rate", xintercept = median(death_rate)), size = 1) + 
  scale_color_manual(values = c("Average Delta Death Rate" = "blue", "Median Delta Death Rate" = "red")) + 
  labs(color = "Summary Statistics") + 
  
  xlab("Delta Death Rates") + 
  ylab("Count") + 
  ggtitle(paste("Disbtribution of Delta Death Rates in Metro Area MN Counties", 
                "\n Average: ", round(mean((prob_2 %>% filter(region == 1))$death_rate), 2), 
                "\n Median: ", round(median((prob_2 %>% filter(region == 1))$death_rate), 2))) + 
  theme_minimal()

```

Delta variant COVID related death rates for outstate counties looks approximately normally distributed with some heavy and obvious
outliers. 

```{r}

ggplot(data = prob_2 %>% filter(region == 0), 
       aes(x = death_rate)) + 
  geom_histogram(binwidth = .1, color = "black", fill = "light yellow") + 
  
  geom_vline(aes(colour = "Average Delta Death Rate", xintercept = mean(death_rate)), size = 1) + 
  geom_vline(aes(colour = "Median Delta Death Rate", xintercept = median(death_rate)), size = 1) + 
  scale_color_manual(values = c("Average Delta Death Rate" = "blue", "Median Delta Death Rate" = "red")) + 
  labs(color = "Summary Statistics") + 
  
  xlab("Delta Death Rates") + 
  ylab("Count") + 
  ggtitle(paste("Disbtribution of Delta Death Rates in Metro Area MN Counties", 
                "\n Average: ", round(mean((prob_2 %>% filter(region == 0))$death_rate), 2), 
                "\n Median: ", round(median((prob_2 %>% filter(region == 0))$death_rate), 2))) + 
  theme_minimal()

dr_out_county <- na.omit(paste(prob_2[prob_2$death_rate > 1.2 & prob_2$region == 0, ]$county))
```
This outlier is a `r dr_out_county` 

Death Rates for this county is `r round(prob_2[prob_2$county == dr_out_county, ]$death_rate,6)[1]`,
and the corresponding vaccination rate is `r round(prob_2[prob_2$county == dr_out_county, ]$v_rate,6)[1]`, which corresponds to the 
`r round(length(which(prob_2$v_rate <= prob_2[prob_2$county == dr_out_county, ]$v_rate[1]))/length(prob_2[prob_2$region == 0, ]$v_rate),4)*100`th quantile 
of vaccination rates for the outstate counties. Perhaps, a lower vaccination causes more COVID related death to happen, which 
we will verify later in the problem. 

```{r}
## summarize the data 
prob_2 %>% 
  group_by(region) %>% 
  summarize(
    n = n(), 
    mean = mean(death_rate),
    median = median(death_rate),
    sd = round(sd(death_rate),2)
  ) %>% 
  mutate(Reg = ifelse(region == 1, "Metro", "Outstate")) %>% 
  select(-region) %>% 
  select(Reg, everything()) %>% 
## pipe it into kale right away
    kbl(booktabs = T, 
        caption = "Death Rates Summary by County Type", 
        col.names = c("Type", "N", "Mean", "Median", "S.D.")) %>%
      kable_styling(latex_options = c("HOLD_position", "striped"))

med_1 <- median((prob_2 %>% filter(region == 1))$death_rate)
med_0 <- median((prob_2 %>% filter(region == 0))$death_rate)

```

We visualize the values of death rate to those expected under the normal distribution. 

```{r}

mse <- sd(prob_2$death_rate)

prob_2_n <- prob_2 %>% arrange(death_rate)

prob_2_n$resid_rank <- as.numeric(rownames(prob_2_n))

N <- nrow(prob_2_n)

prob_2_n$expected_death_rate <- sqrt(mse) * qnorm((prob_2_n$resid_rank - .375)/(N + .25))

corr <- cor(prob_2_n$death_rate, prob_2_n$expected_death_rate)

ggplot(data= prob_2_n, 
       aes(x = expected_death_rate, y = death_rate)) + geom_point() + 
  geom_smooth(method = "lm", color = "red") + 
  ylab("Vacciantion Rate") + 
  xlab("Expected Vacciantion Rate") + 
  ggtitle(paste("Correlation between Observed and Expected", round(cor(prob_2_n$death_rate, prob_2_n$expected_death_rate),3)))+ 
  theme_minimal()

```
```{r}

wil_res <- 
  wilcox.test( 
            y = prob_2[prob_2$region == 1, ]$death_rate, 
            x = prob_2[prob_2$region == 0, ]$death_rate,
            
            conf.int = T,
            conf.level = .95, 
            est_diff = T)

t_test <- 
  t.test(y = prob_2[prob_2$region == 1, ]$death_rate, 
            x = prob_2[prob_2$region == 0, ]$death_rate,
            
            conf.int = T,
            conf.level = .95, 
            est_diff = T)

med_1 <- mean((prob_2 %>% filter(region == 1))$death_rate)
med_0 <- mean((prob_2 %>% filter(region == 0))$death_rate)

```

It appears that there are no violations of normality, other than having a huge outlier on the upper end. 

Test results summary and interpretation for the average death rates between metro and rural ares is given below:

*   Null Hypothesis: $H_0: \mu_{metro \ area} = \mu_{outstate}$ 

*   Test statistic: $H_a: \mu_{metro \ area} \neq \mu_{outstate}$

*   Metro area mean vaccination rate is `r med_1`, while outstate median vaccination mean is `r med_0`

*   Estimated difference is `r med_0-med_1 `, bounded by (`r paste(round(t_test$conf.int[1], 4), ",", round(t_test$conf.int[2], 4))`)

*   Test statistic $T$: `r t_test$statistic`

*   $P(T^* > T) =$ `r round(t_test$p.value,6)`

*   Conclusion: P-value is small, so we can reject the null hypothesis and conclude that the average difference in death rates 
    on the county level is statistically significant between metro and rural areas. On average, we can expect metro area counties 
    to have `r med_0-med_1` COVID related death per 1,000 county residents. 

\newpage 

## 2 - C

```{r}

death_lm <- lm(death_rate ~ v_rate + region, data = prob_2)

```

Model Specification 

$$\Large E[Death \ Rate] = \hat \beta_0 + \hat \beta_1 * X_1 + \hat \beta_2 * X_2 = $$

$$\Large E[Death \ Rate] = \hat \beta_0 + \hat \beta_1 * Vaccination \ Rate + \hat \beta_2 * Metro \ Area \ County \ Indicator$$

**Overall ANOVA test**

```{r}

SSR <- sum(
  (death_lm$fitted.values - mean(prob_2$death_rate))^2
)

SSE <- sum(death_lm$residuals^2)
SSTO <- sum((mean(prob_2$death_rate) - prob_2$death_rate)^2 )
  
df_ssr <- 2 # for 2 predictors 
df_sse <- nrow(prob_2) - 3 # - (2 + 1)

res <- 
  data.frame(
    Source = c("Regression", "Error", "Total"), 
    SSR = c(SSR, SSE, SSTO), 
    DF = c(df_ssr, df_sse, nrow(prob_2)-1)
  )

res$MS <- NA
res[1:2,]$MS <- res[1:2,]$SSR / res[1:2,]$DF

res$`F Statistic` <- NA
res[1,]$`F Statistic` <- round((SSR/df_ssr) / (SSE/df_sse),2)

res$`P(F* > F)` <- NA
res[1,]$`P(F* > F)` <- round(1 - pf((SSR/df_ssr) / (SSE/df_sse), df1 = df_ssr, df2 = df_sse),4)
  
res %>% 
  kbl(booktabs = T, align = 'c') %>% 
  kable_styling(latex_options = c("HOLD_position", "striped"))

```


* Null Hypothesis: $H_0: \beta_1 = \beta_2 = ... = \beta_{p-1}$

* Alternative Hypothesis: $H_a:$ Not all coefficients $\beta_i$ are zero

* $F-$statistic: `r round((SSR/df_ssr) / (SSE/df_sse),2)`

* Cutoff $F^*$-statistic: `r round(qf(1-.05, df1 = df_ssr, df2 = df_sse),4)`

* So, $F < F^*$, therefore we do not have enough evidence to reject the null hypothesis to conclude that some or all 
  coefficients $\beta_i$ are consistently different from zero. 

* Moreover, $P(F^* > F) =$ `r round(1 - pf((SSR/df_ssr) / (SSE/df_sse), df1 = df_ssr, df2 = df_sse),4)`
  
* Conclusion: There is enough statistical evidence to reject the null hypothesis and conclude that at least one coefficient is 
different from 0. Therefore, we will evaluate model output and find what explains variation in death rates on the county level.  

Table below shows **Regression Coefficients** and model summary. Like we expected, these coefficients are small because the scale and 
range of predictors and response variable are not the same. 


```{r}
res_reg <- data.frame(summary(death_lm)$coefficients)
res_reg$var <- rownames(res_reg)
rownames(res_reg) <- NULL
res_reg <- res_reg %>% select(var, everything())
res_reg <-
  res_reg %>% mutate_at(vars(Estimate, `Std..Error`, t.value, `Pr...t..`),
                                 funs(round(., 6)
                                      )
                                 )

colnames(res_reg) <- c("Predictor", "Estiamte", "Standard Error", "T Value", "P value")
res_reg %>%
  kbl(booktabs = T, align = c('l','c', 'c', 'c', 'c')) %>%
  kable_styling(latex_options = c("striped", "HOLD_position"))

est <- res_reg[res_reg$Predictor == "region", ]$Estiamte
se <- res_reg[res_reg$Predictor == "region", ]$`Standard Error`

tv <- res_reg[res_reg$Predictor == "region", ]$`T Value`
pv <- res_reg[res_reg$Predictor == "region", ]$`P value`

```

* R square `r round(summary(death_lm)$r.square,4)`

* Adjusted R Square `r round(summary(death_lm)$adj.r.squared,4)`

Note that $X_2$ is the metro/rural area flag indicator variable, so we conduct a formal t-test for this variable below: 

*   Null Hypothesis: $H_0: \hat \beta_2 = 0$

*   Alternative Hypothesis: $H_a:  \hat \beta_2 \neq 0$

*   Test statistic $T:$ `r tv`

*   $P(t^* > t) =$ `r pv`

*   Conclusion: the p-value is quite large, so we can't reject the null hypothesis. The only useful predictor is the vaccination rate, 
    and it is a statistically significant predictor. This is a bit contradictory with the results that we saw in the individual t-tests. 
    So far, we know that vaccination rates are higher for metro counties, while death rates are lower for the metro areas. 
    But the regression model we used does not support the statement that death rates meaningfully differ between the two types of counties. 
    Therefore, we can conclude that vaccination rates is either: 
    
      + a much stronger predictor than a county status 
      
      + a more likely conclusion is that the two are correlated, and the effect of vaccination rate already includes most of county effect 
        into its coefficient 

**Interpretation of  coefficient**

Metro Area is expected to have `r abs(round(abs(est),4))` less COVID related deaths per 1,000 residents 

**C.I.** 

```{r}
est <- res_reg[res_reg$Predictor == "region", ]$`Estiamte`

se <- res_reg[res_reg$Predictor == "region", ]$`Standard Error`

ci <- data.frame(confint(death_lm))

ci$name <- rownames(ci)

est_lb <- round(ci[ci$name == "region", ]$X2.5.., 6)
est_ub <- round(ci[ci$name == "region", ]$X97.5.., 6)
  
```

C.I. for the estimate `r est` with a `r se` standard error is (`r est_lb`, `r est_ub`). 
Note now the C.I. is pretty equally balanced around the estimate that is very close to 0, which gives us an even higher degree of 
certainty that the difference between the metro and rural counties is more likely to be 0, after adjusting for vaccination rate. 


## 2 - D

If we are suggested to use a weighted least squares linear model, then we must have a problem with constant variance assumption. 

Let's look at the plot of studentized residuals versus predicted values.

```{r}
##################
# ORIGINAL MODEL FOR COMPARISON

resid_plot_df <-
  data.frame(
    resid = rstandard(death_lm),
    fit = death_lm$fitted.values
  )

ggplot(data = resid_plot_df,
       aes(x = fit,
           y = resid)) +
    geom_point() +

    geom_smooth(aes(color = "Smooth Trend Line")) +
    geom_smooth(method = "lm", se = F, aes(color = "Fitted Regression Line")) +
    scale_color_manual(values = c("Smooth Trend Line" = "blue", "Fitted Regression Line" = "red")) +

    theme_minimal() +
    xlab("Fitted Values") +
    ylab("Residuals") +
    ggtitle("Predicted Values And Studentized Residuals \n From Unweighted Linear Model") +
    labs(color = "Line Type")

```

Indeed, we can see that variance of residuals is not constant for different levels of predicted values. 
There is also a very obvious outlier here: a `r prob_2[which(rstandard(death_lm) > 4), ]$county` county. 
We have a megaphone type
pattern, and we have a variety of ways how we can construct weights for the updated regression model. 

1. A remedial measure from chapter 11 of the textbook suggests that the can regress absolute values of residuals on the fitted values and 
  obtain weights through the following variance function: 
  
  + So we have a regression model $\large |\epsilon_i| = \hat \beta_0 + \hat \beta_1 * \hat Y_i + error_i$
  
  + Weights for each observation is given by $\Large w_i = \frac{1}{E[|\epsilon_i|]}$
  
2. Another remedial measure is to fit the variance function using a different way: 

  + Regression model for the variance function is $\large \epsilon_i^2 = \hat \beta_0 + \hat \beta_1 * Vaccination \ Rate +\hat \beta_2 * Metro \ Area\ Flag + error_i$
  
  + Weights for each observation is given by $\Large w_i = \frac{1}{E[\epsilon_i^2]}$

**Method 1**

We re-fit a linear model with weights using method 1 and provide a model summary below: 

* `v_rate` is a variable for vaccination rates 
* `region` is an indicator variable comparing metro areas with the outstate counties, which is a reference level

```{r}

# another implementation: this is the correct version I would say. 

weights3 <- 1 / lm((death_lm$residuals)^2 ~ death_lm$fitted.values)$fitted.values^2 # best yet after 2 
weights6 <- 1 / abs(lm((death_lm$residuals)^2 ~ v_rate + region, data = prob_2)$fitted.values) # a little better too 

# 3 and 6 are the two best implementations, so I will do both of them and then compare and contrast and will provide my 
# answer like that cause I really do not know what to do 


death_lm_3 <- 
  lm(death_rate ~ v_rate + region, data = prob_2, weights = weights3)

res_reg <- data.frame(summary(death_lm_3)$coefficients)
res_reg$var <- rownames(res_reg)
rownames(res_reg) <- NULL
res_reg <- res_reg %>% select(var, everything())
res_reg <-
  res_reg %>% mutate_at(vars(Estimate, `Std..Error`, t.value, `Pr...t..`),
                                 funs(round(., 6)
                                      )
                                 )

colnames(res_reg) <- c("Predictor", "Estiamte", "Standard Error", "T Value", "P value")
res_reg %>%
  kbl(booktabs = T, align = c('l','c', 'c', 'c', 'c')) %>%
  kable_styling(latex_options = c("striped", "HOLD_position"))
```

After re-weighting the model using method 1, model estimates changed compared with the original model. Key differences are given below: 

* R square changed from `r round(summary(death_lm)$r.square,4)` to `r round(summary(death_lm_3)$r.square,4)`

* Adjusted R Square changed from `r round(summary(death_lm)$adj.r.squared,4)` to `r round(summary(death_lm_3)$adj.r.squared,4)`

* Vaccination rates is still a statistically significant predictor, and the coefficient did not change meaningfully. 

* County status' p-value dropped meaningfully, bringing closer to the level that will make us believe that the death rates are potentially 
  higher in the metro area, on average, after adjusting for vaccination rates. This is quite a contradictory statement, 
  however, we know the confidence interval is centered at around 0 for this predictor, therefore interpretation of this coefficient can 
  produce misleading results 
  
We also evaluate the plot of studentized residuals with the fitted values from the regression model with weighting described in method 1.

```{r}
resid_plot_df <- 
  data.frame(
    resid = rstandard(death_lm_3), 
    fit = death_lm_3$fitted.values
  )

ggplot(data = resid_plot_df, 
       aes(x = fit, 
           y = resid)) + 
    geom_point() + 
    
    geom_smooth(aes(color = "Smooth Trend Line")) + 
    geom_smooth(method = "lm", se = F, aes(color = "Fitted Regression Line")) + 
    scale_color_manual(values = c("Smooth Trend Line" = "blue", "Fitted Regression Line" = "red")) + 
    
    theme_minimal() + 
    xlab("Fitted Values") + 
    ylab("Residuals") + 
    ggtitle("Predicted Values And Studentized Residuals From Method 1") +  
    labs(color = "Line Type")

```

We can see that, overall, the variance of the residuals is in more balance. Yes, there is still some increase in residual values as fitted 
values increase, but it stops at around fitted values of 0.3 for positive values of residuals. Negative residuals seem to be at around 
-1 to -1.2 bound. One noticeable problem is the introduction of two more new outliers, where the model predicts a very low fitted value, 
but the actual observed value differs meaningfully from the prediction. 

**Method 2**

I encountered one issue when implementing method 2. After fitting a regression model with squared residuals as the response variable, and 
vaccination rates and county status as predictors, I obtained fitted values from the model, and some of them were negative, as 
evidenced by the summary output below: 

```{r, echo= T}
weights6 <- 1 / lm( (death_lm$residuals)^2 ~  v_rate + region, data = prob_2)$fitted.values  
summary(weights6)
```

It should not be the case as variance is always greater than 0. There are `r length(which(weights6 < 0))` counties where the output of the 
variance function is below 0. Those values are: `r weights6[weights6 < 0]`, so, for this exercise, I will take an absolute value of 
these values to make sure all inverse variance weights are above 0. 

We re-fit a linear model with weights using method 1 and provide a model summary below: 

* `v_rate` is a variable for vaccination rates 
* `region` is an indicator variable comparing metro areas with the outstate counties, which is a reference level

```{r}

# another implementation: this is the correct version I would say. 

weights3 <- 1 / lm((death_lm$residuals)^2 ~ death_lm$fitted.values)$fitted.values^2 # best yet after 2 
weights6 <- 1 / abs(lm((death_lm$residuals)^2 ~ v_rate + region, data = prob_2)$fitted.values) # a little better too 

# 3 and 6 are the two best implementations, so I will do both of them and then compare and contrast and will provide my 
# answer like that cause I really do not know what to do 


death_lm_6 <- 
  lm(death_rate ~ v_rate + region, data = prob_2, weights = weights6)

res_reg <- data.frame(summary(death_lm_6)$coefficients)
res_reg$var <- rownames(res_reg)
rownames(res_reg) <- NULL
res_reg <- res_reg %>% select(var, everything())
res_reg <-
  res_reg %>% mutate_at(vars(Estimate, `Std..Error`, t.value, `Pr...t..`),
                                 funs(round(., 6)
                                      )
                                 )

colnames(res_reg) <- c("Predictor", "Estiamte", "Standard Error", "T Value", "P value")
res_reg %>%
  kbl(booktabs = T, align = c('l','c', 'c', 'c', 'c')) %>%
  kable_styling(latex_options = c("striped", "HOLD_position"))
```


After re-weighting the model using method 1, model estimates changed compared with the original model. Key differences are given below: 

* R square changed from `r round(summary(death_lm)$r.square,4)` to `r round(summary(death_lm_6)$r.square,4)`

* Adjusted R Square changed from `r round(summary(death_lm)$adj.r.squared,4)` to `r round(summary(death_lm_6)$adj.r.squared,4)`

* Vaccination rates is still a statistically significant predictor, and the coefficient did not change meaningfully. 


```{r}
resid_plot_df <- 
  data.frame(
    resid = rstandard(death_lm_6), 
    fit = death_lm_6$fitted.values
  )

ggplot(data = resid_plot_df, 
       aes(x = fit, 
           y = resid)) + 
    geom_point() + 
    
    geom_smooth(aes(color = "Smooth Trend Line")) + 
    geom_smooth(method = "lm", se = F, aes(color = "Fitted Regression Line")) + 
    scale_color_manual(values = c("Smooth Trend Line" = "blue", "Fitted Regression Line" = "red")) + 
    
    theme_minimal() + 
    xlab("Fitted Values") + 
    ylab("Residuals") + 
    ggtitle("Predicted Values And Studentized Residuals From Method 2") +
    labs(color = "Line Type")
```


We can see that, overall, the variance of the residuals is in more balance. There is more increase in residual values as fitted 
values increase for positive values of residuals, compared with method 1. We still could not deal with one outlier that is more than 4 
studentized residuals away. Overall, lower bound for residuals does not exhibit any suggestions that there is a violation fo variance 
assumption, but positive residuals still have non-constant variance, it appears. 

**Overall**, the two methods of weighted linear regression produce three sets of estimates that agree with each other. All three models 
showed that higher vaccination rates caused lower death rates after adjusting for metro/rural area status. 

Metro/Outstate indicator was not statistically significant in any of the models. The point estimate for coefficient fluctuated above and 
below zero in three models, but none of the test statistics and p-values showed enough evidence that this variable is a useful 
predictor. 


---
title: "Homework 5"
author: "Denis Ostroushko"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    extra_dependencies: ["float"]
editor_options: 
  markdown: 
    wrap: 72
---

```{r, echo = F}
knitr::opts_chunk$set(echo = T, message = F, warning = F, fig.pos = "!H")
options(scipen=999)
```

```{r, include=F}
library(tidyverse)
library(kableExtra)
library(readxl)
library(gridExtra)
library(ggeffects)
```

```{r, eval=F, echo = T}
library(tidyverse)
library(kableExtra)
library(readxl)
library(gridExtra)
library(ggeffects)
```

# 10.2

We enter the data below. We display the code in case we make a error and need to trace the mistake back to the origin. 

```{r}
#put in the data 

dose <- c(rep(5.76,3),
          rep(9.6, 5), 
          rep(16, 4), 
          rep(32.4, 3), 
          rep(54, 3), 
          rep(90, 4), 
          rep(150, 5))

treat <- c(rep("Vitamin D3", 12), 
           rep("Cod-liver Oil", 15))

response <- c(33.5, 37.3, 33, 
              36.2, 35.6, 36.7, 37, 39.5, 
              41.6, 37.9, 40.5, 42,
              32, 33.9, 30.2, 
              32.6, 37.7, 36, 
              35.7, 42.8, 38.9, 40.3, 
              44, 43.3, 38.4, 44.2, 43.7)

vit_data <- data.frame(dose, response, treat)

```

## 10.2 - A

In this section we need to use scatter plots and test to verify that the response measurement fits better against the dose on the logarithmic 
scale rather than the original one. We will also use this as a chance to use an F test for lack of fit. 

### Visual Examination of Dose Scale Against Respose

First, we begin with the visual examination of the relationship between the response and dose on different scales. 
Two plots are presented below, type of fit is stated in the title. 

```{r, fig.align='center', echo=FALSE}

g1 <- ggplot(data = vit_data, 
             aes(x = dose, 
                 y = response, 
                 group = treat)) + 
  
  geom_point() +
  geom_smooth(aes(color = "Smooth Trend Line"), se = F) + 
  stat_smooth(aes(color = "Fitted Regression Line"), method = "lm", se = F) + 

  scale_color_manual(
    name = "Line Type", values = c("Smooth Trend Line" = "blue", "Fitted Regression Line" = "red")
  ) +

  xlab("Experimental Dose") + 
  ylab("Response") + 
  ggtitle("Response vs Dose \n on the Original Scale")  + 
  theme_minimal() + 
  theme(legend.position="bottom", 
        legend.title = element_text(size = 8), 
        legend.text = element_text(size = 10)
        ) + 
  guides(color = guide_legend(nrow=2, byrow=TRUE))

g2 <- ggplot(data = vit_data, 
             aes(x = log(dose), 
                 y = response, 
                 group = treat)) + 
  geom_point() + 
  geom_smooth(se = F, aes(color = "Smooth Trend Line")) + 
  stat_smooth(se = F, method = "lm", aes(color = "Fitted Regression Line")) + 
  
  scale_color_manual(
    name = "Line Types", values = c("Smooth Trend Line" = "blue", "Fitted Regression Line" = "red" )) + 
      
  xlab("Experimental Dose") + 
  ylab("Response") + 
  ggtitle("Response vs Dose \n on the Logarithmic Scale") + 
  theme_minimal() + 
  theme(legend.position="bottom", 
        legend.title = element_text(size = 8), 
        legend.text = element_text(size = 10)
        ) + 
  guides(color = guide_legend(nrow=2, byrow=TRUE))


grid.arrange(g1, g2, nrow = 1)

```

Right away we can see the two main problems with fitting the model to the original scale of dose: 

1. We are conducting a Parallel-line assays analysis, and as we can see, the slopes for two groups are not parallel. Therefore, there is an 
  effect modification present, an interaction between treatment type and a dose given. So, it will be inappropriate to fit the model with 
    one common slope and two different intercepts to such data. We can see that this is not the case when we obtain look at the relationship 
    between dose on the logarithmic scale and the response. 
    
2. There is potentially an issue with the linearity of the fit. It is hard to tell if the Vitamin D3 Dose is indeed linearly related to the 
    response without zooming in, but there is clearly an issue for the Cod-Liver treatment. The relationship is curved, which will 
    cause issues with the residuals. This issue goes away when we consider dose on the logarithmic scale. The smooth line 
    dips below and above the regression line, in a random fashion. 

### Lack of Fit Test for Original-Dose-Scale Based Model

Now we take a chance to conduct a statistical F-test and assess model's fit. Due to models being multivariate, I do not present calculation 
of the test statistic by hand and use built in `R` functions instead. Recall that the idea of the test is to see if the overall fit of the model 
is good for each replicate level, or quasi-level, of the predictor variable. If the group-wise deviations accumulate to a greeter total error than
the pure error, i.e. SSE, than the model does not fit well for certain groups, or levels, of the predictor variables, and we need to 
consider a different model. 

We first conduct a lack of fit test for a regression model against the original dose values. 

```{r}
reduced <- lm(response ~ dose + treat, data = vit_data)
full <- lm(response ~ 0 + as.factor(dose) + treat,data = vit_data)

res <- data.frame(anova(reduced, full) )

res$name <- c("Linear Fit","Within Group Fit")

res <- res %>% dplyr::select(name, everything())

colnames(res)[1] <- "Model Type"

res %>% 
  kbl(booktabs = T, align = 'c') %>% 
  kable_styling(latex_options = c("HOLD_position", "striped"))
```

Using code in the chunk above, we get all the data and statistics to conduct a test. We assume that the function of response is a linear model. 

* Overall, Dose and Treatment explain `r paste0(round(summary(reduced)$r.squared, 4) * 100, "%")` of variation in response 
  measurements 

* Null Hypothesis: $H_0: E[Y] = \beta_0 + \beta_1 * Dose + \beta_2 * Treatment$

* Alternative Hypothesis: $H_a: E[Y] \neq \beta_0 + \beta_1 * Dose + \beta_2 * Treatment$

* Test Statistic: $F=$ `r round(res$F[2], 4)`

* $P(F^* > F)=$ `r res[2,7]`

* Conclusion: P-value is too close to 0.05. Therefore, we will reject the null hypothesis and conclude that response, Y, can't be reasonably 
  expressed as a linear combination of dose and treatment levels. We need a different model or different scales of predictors. 

We can also see that the fit is bad on the graph below. When fitting the model with the common slope, there is too much error and variation around
the regression line for Vitamin D3 doses. This is something that we can tie back to the initial scatter plot, where we saw that we need two 
different slopes for two different treatments. 

```{r, echo=F}

my_pred <- ggpredict(reduced, terms = c("dose", "treat"))

obs <- vit_data %>% 
  select(dose, treat) %>% unique() %>% 
  rename('group' = 'treat', 
          'x' = 'dose')
  
my_pred <- 
  my_pred %>% 
  inner_join(obs, by = c('group', 'x'))
        
vit_data_p <- 
  vit_data %>% 
    rename(
      'x' = 'dose', 
      'group' = 'treat', 
      'predicted' = 'response'
    )
        
ggplot(data = vit_data_p, 
             aes(x = x, 
                 y = predicted, 
                 group = group, 
                 shape = group)) + 
  
  geom_point()+
  
  geom_line(data = my_pred, aes(x = x, y = predicted, color = group)) + 
  
  xlab("Experimental Dose") + 
  ylab("Response") + 
  ggtitle("Response vs Dose \n on the Original Scale")  + 
  theme_minimal()

```


### Lack of Fit Test for Logarithmic-Dose-Scale Based Model

Now we will consider a fit of response against treatment level and log-transformed doses. Again, we use `R` code to conduct the test and 
evaluate hypotheses. 

```{r}
vit_data$log_dose <- log(vit_data$dose)

reduced <- lm(response ~ log_dose + treat, data = vit_data)
full <- lm(response ~ 0 + as.factor(log_dose) + treat,data = vit_data)

res <- data.frame(anova(reduced, full) )

res$name <- c("Linear Fit","Within Group Fit")

res <- res %>% dplyr::select(name, everything())

colnames(res)[1] <- "Model Type"

res %>% 
  kbl(booktabs = T, align = 'c') %>% 
  kable_styling(latex_options = c("HOLD_position", "striped"))
```

* Overall, Log - Dose and Treatment explain `r paste0(round(summary(reduced)$r.squared, 4) * 100, "%")` of variation in response 
  measurements 
  
* Null Hypothesis: $H_0: E[Y] = \beta_0 + \beta_1 * Log - Dose + \beta_2 * Treatment$

* Alternative Hypothesis: $H_a: E[Y] \neq \beta_0 + \beta_1 * Log - Dose + \beta_2 * Treatment$

* Test Statistic: $F=$ `r round(res$F[2], 4)`

* $P(F^* > F)=$ `r res[2,7]`

* Conclusion: The p-value is very large, meaning that we do not have enough evidence to reject the null hypothesis. Therefore, we can use 
  this model for the Parallel-line assays analysis. 

We can see that the fit is so much better here. Parallel slopes fit their respective groups very well. Variance of data points around the 
regression lines is reduced, and visually appears constant. So, we will use this model to complete to the rest of this problem. 

```{r, echo  = F}

my_pred <- ggpredict(reduced, terms = c("log_dose", "treat"))

obs <- vit_data %>% 
  select(dose, treat) %>% unique() %>% 
  rename('group' = 'treat', 
          'x' = 'dose') %>% 

  mutate(x = log(x))
  
my_pred <- 
  my_pred %>% 
  inner_join(obs, by = c('group', 'x'))
        
vit_data_p <- 
  vit_data %>% 
    rename(
      'x' = 'dose', 
      'group' = 'treat', 
      'predicted' = 'response'
    )
        
ggplot(data = vit_data_p, 
             aes(x = log(x), 
                 y = predicted, 
                 group = group, 
                 shape = group)) + 
  
  geom_point()+
  
  geom_line(data = my_pred, aes(x = x, y = predicted, color = group)) + 
  
  xlab("Experimental Dose") + 
  ylab("Response") + 
  ggtitle("Response vs Dose \n on the Logarithmic Scale")  + 
  theme_minimal()
```


## 10.2 - B

From the lecture notes we know that parallel-line assays are those in which the response is linearly related to the log dose. 
If the response is linearly related to log dose, then the coefficient should be statistically different from 0. Using a model from the 
previous section, we can validate this. Looking at the summary table for the model, we can conduct a statistical test for the coefficient of 
log-dose. 

```{r, echo = F}

vit_data$treat <- factor(vit_data$treat , levels = c("Vitamin D3", "Cod-liver Oil"))

full_lm <- lm(response ~ log(dose) + treat, data = vit_data)

sum_data <- data.frame(summary(full_lm)$coefficients)

sum_data$names <- c("Intercept", "Log - Dose", "Cod - Liver Oil Treatment")

rownames(sum_data) <- NULL

sum_data <- sum_data %>%  dplyr::select(names, everything())

round_3 <- function(x){round(x,3)}
sum_data[,2:5] <- lapply(sum_data[,2:5], round_3)

colnames(sum_data) <-c("Model Term", "Estimate", "Std. Error", "T-value", "P-value") 



sum_data %>% 
  kbl(booktabs = T, align = 'c') %>% 
  kable_styling(latex_options = c("striped", "HOLD_position"))
```

* Null Hypothesis: $\hat \beta_{log-dose} = 0$

* Alternative Hypothesis: $\hat \beta_{log-dose} \neq 0$

* Test Statistic: `r sum_data[2,4]`

* $P(t^* > t)=$ `r sum_data[2,5]`

* Conclusion:  P-value is less than 0.05, so we reject the null hypothesis in favor of the alternative hypothesis. We have enough statistical 
evidence to conclude that the response is linearly related to the log-dose variables. 


## 10.2 - C

Using model summary table, we know that the two estimates are: 

* $\hat \beta_1 =$ `r round(coefficients(full_lm)[2], 2)`

* $\hat \beta_2 =$ `r round(coefficients(full_lm)[3], 2)`

Code below calculates $m = log[p]$

```{r}
rel_pot <- coefficients(full_lm)[3] / coefficients(full_lm)[2]
```

* From slide 11, we obtain Relative potency = $m = log [p] = \frac{\hat \beta_2}{\hat \beta_1}$ = `r round(rel_pot, 4)`

## 10.2 - D

Formula for variance and standard error of the estimate is much bigger, and requires that we obtain more estimates from the model. 
We save down  $\hat \beta_1$ and $\hat \beta_2$ using the model we fit and used in the previous sections. Also, using this model we obtain a 
variance-covariance matrix for the estimates. Code chunk below obtains and saves all estimates that we need. 

```{r}

beta_1 <- coefficients(full_lm)[2]
beta_2 <- coefficients(full_lm)[3]

var_beta_1 <- vcov(full_lm)[2,2] # beta_1 variance 
var_beta_2 <- vcov(full_lm)[3,3] # beta_2 variance 

cov_beta_12 <- vcov(full_lm)[2,3] #covariance of beta_1 and beta_2 

```

We copy down a formula from lecture Slide 17: 

$$\Large Var(m) = \frac{\hat \beta_2^2}{\hat \beta_1^4} \times Var(\hat \beta_1) + 2(-\frac{\hat \beta_2}{\hat \beta_1^2}) \times  (\frac{1}{\hat \beta_1}) \times Cov(\hat \beta_1, \hat \beta_2) + \frac{1}{\hat \beta_1^2} \times Var(\hat \beta_2)$$
For this calculation we have the following estimates: 

* $\hat \beta_1 = b_1 =$ `r round(beta_1, 5)`

* $\hat \beta_2 = b_2 =$ `r round(beta_2, 5)`

* $Var(\hat \beta_1) = Var(b_1) =$ `r round(var_beta_1, 5)`

* $Var(\hat \beta_2) = Var(b_2) =$ `r round(var_beta_2, 5)`

* $Cov(\hat \beta_1, \hat \beta_2) = Cov(b_1, b_2) =$ `r round(cov_beta_12, 5)`

Calculation of the estimate is given below: 
```{r}
Var_m <- 
  (beta_2 ^ 2)/(beta_1 ^ 4) * var_beta_1 + 
    2 * (-1) * (beta_2 / beta_1^2 ) * (1/beta_1) * cov_beta_12 + (1/(beta_1^2)) * var_beta_2
```

So, $Var(m)=$ `r round(Var_m,5)`, and the standard error is $se(m) = \sqrt{Var(m)}=$ `r round(sqrt(Var_m),5)`

# 11.1 

```{r, echo = F}
cig <- read_xls("/Users/denisostroushko/Desktop/UofM MS/MS Fall 2022/Puhb 7405/Data Sets/Cigarettes.xls")
colnames(cig) <- c("age", "gender", "cpd", "carbon_mono", "cotinine", "nnal")

cig <- cig %>% dplyr::select(nnal, cpd, age, gender )
```

## 11.1 - A

In order to decompose SSR into Extra Sum of Squares we will do in sequence, fitting models one by one and calculating SSR along the way. 
We need the following components: 

* Full Regression SSR, that involves all 3 predictors: $SSR(CPD, Age, Gender)$ and $df = p-1 = 4-1 =3$

* $SSR(CPD)$ and $df = p_{cpd} - 1 = 2- 1= 1$, because we have two model estimates that helped us obtain this SSR

* $SSR(Age|CPD)$ and $df = 1$, for one extra predictor that we include 
  + $SSR(Age|CPD) = SSR(Age, CPD) - SSR(CPD)$
    
* $SSR(Gender|CPD, Age)$ and $df = 1$, again, for the extra predictor 
  + $SSR(Gender|CPD, Age) = SSR(Age, CPD, Gender) - SSR(Age, CPD)$ 
  
Code below shows how we fit models one and by and save down SSR and Extra SSR along the way

```{r}
full_model <- lm(nnal ~ cpd + age + gender, data = cig)
SSR_full <- sum((mean(cig$nnal) - full_model$fitted.values)^2) # full regression SSR 

cpd_model <- lm(nnal ~ cpd, data = cig)
SSR_cpd <- sum((mean(cig$nnal) - cpd_model$fitted.values)^2) # SSR(X_1)

age_model <- lm(nnal ~ age, data = cig)
SSR_age <- sum((mean(cig$nnal) - age_model$fitted.values)^2) # SSR(X_2)

cpd_age_model <- lm(nnal ~ cpd + age, data = cig)
SSR_cpd_age <- sum((mean(cig$nnal) - cpd_age_model$fitted.values)^2) # (SSR X_1, X_2)

SSR_age_given_cpd <- SSR_cpd_age - SSR_cpd # SSR(X_2 | X_1)
SSR_cpd_given_age <- SSR_cpd_age - SSR_age # SSR(X_1 | X_2)

SSR_gender_given_cpd_age <- SSR_full - SSR_cpd_age # SSR(X_3 | X_1, X_2) = SSR(X_3 | X_2, X_1)

```

We also save down residual sum of squares and total sum of errors 

```{r}

SSE <- sum(full_model$residuals^2)

SSTO <- sum((mean(cig$nnal) - cig$nnal)^2)

```

\newpage

And we finally can put together a table that we will use later in the analysis

```{r, echo=F}
anova_tab <- 
  data.frame(
    Source = c("CPD + Age + Gender", "CPD", "Age|CPD", "Gender|Age, CPD", "Residual Error", "Total Error"), 
    SS = c(SSR_full, SSR_cpd, SSR_age_given_cpd, SSR_gender_given_cpd_age, SSE, SSTO), 
    DF = c(3,1,1,1,nrow(cig)-4,nrow(cig)-1)
  )

anova_tab$MS <- anova_tab$SS / anova_tab$DF

anova_tab$MS[6] <- NA

anova_tab %>% 
  kbl(booktabs = T) %>% 
  kable_styling(latex_options = c("striped", "HOLD_position")) %>% 
  pack_rows("Extra SS", 2, 4) %>% 
  pack_rows("Error", 5, 6)
```

We can also double check our calculation using a build in `anova()` function. 

```{r}
anova_built_in <- data.frame(anova(full_model))
anova_built_in$Variable <- c("CPD", "Age", "Gender", "Residuals")

rownames(anova_built_in) <- NULL

anova_built_in <- anova_built_in %>% select(Variable, everything())

anova_built_in %>% 
  kbl(booktabs = T, align = 'c') %>% 
  kable_styling(latex_options = c("HOLD_position", "striped"))
```

As we can see, Sum of Squares and Mean Square, match with the table that we have produced

## 11.1 - B

To test we need to get a few values for the F statistic

* We already have extra sum of squares $SSR(Gender|CPD, Age)$

* We also have $SSE(Gender, Age, CPD)$ 

* $F$ - statistic is then:

$$\large \frac{\frac{SSR(Gender|CPD, Age)}{1}}{\frac{SSE(Gender, Age, CPD)}{n-4}}$$
Hypothesis and test results are given below: 

* Null Hypothesis: $H_0: \hat \beta_{gender} = 0$

* Alternative  Hypothesis: $H_0: \hat \beta_{gender} \neq 0$

* $F-$ statistic: `r round((SSR_gender_given_cpd_age/1)/(SSE/(nrow(cig)-4)), 4)`

* $P(F^* > F) =$ `r 1 - pf((SSR_gender_given_cpd_age/1)/(SSE/(nrow(cig)-4)), 1, nrow(cig)-4)`

* For comparison, here is a model summary that provides a t-test for Gender covariate:

```{r, echo = F}
full_lm <- lm(nnal ~ cpd + age + gender, data = cig)

sum_data <- data.frame(summary(full_lm)$coefficients)

sum_data$names <- c("Intercept", "CPD", "Age", "Gender")

rownames(sum_data) <- NULL

sum_data <- sum_data %>%  dplyr::select(names, everything())

round_3 <- function(x){round(x,3)}
sum_data[,2:5] <- lapply(sum_data[,2:5], round_3)

colnames(sum_data) <-c("Model Term", "Estimate", "Std. Error", "T-value", "P-value") 



sum_data %>% 
  kbl(booktabs = T, align = 'c') %>% 
  kable_styling(latex_options = c("striped", "HOLD_position"))
```

* Note how test statistic $F$ equals squared $t$ statistic from the model summary table above. P-value for the t-test also matches a p-value 
from the F-test that we conducted, which is obviously to be expected. 

* Conclusion: Since p-value is above 0.05, we can not reject the Null hypothesis and conclude that the coefficient for gender is statistically 
  different from 0. Therefore, we also can not conclude that the inclusion of gender into the model
  after cpd and age meaningfully contributes to the proportion of the variation in NNAL values that the model is explains. 

## 11.1 - C

To test we need to get a few values for the F statistic

* We already have extra sum of squares $SSR(Gender, Age|CPD) = SSR(Age, Gender, CPD) - SSR(CPD)$

```{r, echo = F}
SSR_age_gender_given_cpd = SSR_full - SSR_cpd
```

* We also have $SSE(Gender, Age, CPD)$ 

* $F$ - statistic is then:

$$\Large \frac{\frac{SSR(Gender, Age|CPD)}{2}}{\frac{SSE(Gender, Age, CPD)}{n-4}}$$
Hypothesis and test results are given below: 

* Null Hypothesis: $H_0: \hat \beta_{gender} = \hat \beta_{age} = 0$

* Alternative  Hypothesis: $H_a: \hat \beta_{gender}$ and $\hat \beta_{age}$ are not all 0

* $F-$ statistic: `r round((SSR_age_gender_given_cpd/2)/(SSE/(nrow(cig)-4)), 4)`

* $P(F^* > F) =$ `r 1 - pf((SSR_age_gender_given_cpd/1)/(SSE/(nrow(cig)-4)), 1, nrow(cig)-4)`

* Conclusion: the p-value is close to 0.05, but still twice as big as our accepted confidence level. Therefore, we do not have enough 
  evidence to conclude that the inclusion of both age and gender after cpd meaningfully contributes to the proportion of variation of 
  the response variable that are able to explain.

## 11.1 - D

Yes, it is always the case, because the order of the variables is arbitrary. 

For example, in this problem we have 

* $SSR(X_1) = SSR(CPD)$ = `r SSR_cpd`

* $SSR(X_2) = SSR(Age)$ = `r SSR_age`

* $SSR(X_2|X_1) = SSR(Age|CPD) =$ `r SSR_age_given_cpd`

* $SSR(X_1|X_2) = SSR(CPD|Age) =$ `r SSR_cpd_given_age`

* Now we can show that 

  $SSR(X_2|X_1) + SSR(X_1)  =$
  
  `r SSR_age_given_cpd` + `r SSR_cpd` = 
  
  `r SSR_age` + `r SSR_cpd_given_age` = 
  
  $SSR(X_1|X_2) + SSR(X_2)$
  
  
  




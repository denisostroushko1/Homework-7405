---
title: "Homework 1"
author: "Denis Ostroushko"
date: '`r as.Date(Sys.Date())`'
output: 
  pdf_document:
    extra_dependencies: ["float"]
editor_options: 
  markdown: 
    wrap: 72
---

```{r, echo = F}
knitr::opts_chunk$set(echo = F, message = F, warning = F, fig.pos = "!H", fig.height=4, fig.width=7, fig.align='center')
options(scipen=999)
```

```{r, include = F, echo  = F}
source('/Users/denisostroushko/Desktop/UofM MS/MS Fall 2022/Puhb 7405/Package master list .R')
```

# 2.1

```{r}

e_cig <- read_xlsx("/Users/denisostroushko/Desktop/UofM MS/MS Fall 2022/Puhb 7405/Data Sets/E-CIG-1.xlsx", 
                   sheet = "Data_cleaned_for_analysis")
```

```{R data-set-summary}

n_rows <- nrow(e_cig)

unique_ID <- length(unique(e_cig$ID))

n_smokers <- length(unique(e_cig[e_cig$Group == "E-CIG" , ]$ID))
n_not_smokers <- unique_ID - n_smokers

smokers_obs <- nrow(e_cig[e_cig$Group == "E-CIG" , ])
non_smoker_obs <- n_rows - smokers_obs
```

In this data we have:

-   `r n_rows` Total observations for `r unique_ID` unique participants
    in the study

-   `r smokers_obs` observations for `r n_smokers` unique E-cig smokers

-   `r non_smoker_obs` observations for `r n_not_smokers` unique non-smokers
    smokers

The goal of the study is to compare the level of two biomarkers between
those who do and do not use E-cigarettes.

First biomarker is CEMA. CEMA is a highly reliable urinary biomarker to
identify users of combustive tobacco products such as cigarettes as
opposed to users of non-combustive products, medicinal nicotine, or
nonusers of tobacco products.

Second, HOP1, referred to as 1-HOP in the literature, is another urinary
biomarker.

We begin our comparison of groups with the two sample t-test for both
measurements.

## (1) Two-sample t-test

### Test of average CEMA levels

Before conducting a two-sample t-test, let's summarize the data. We need
to know the average value of CEMA in two groups, standard deviation,
sample size(number of measurements, not number of unique participants),
and a standard error for the mean. We will calculate standard error
using s.d. and sample size.

Table below presents these statistics.

```{r}
sum_tab <- 
  e_cig %>% 
    group_by(Group) %>% 
    summarize(n = n(), 
              mean = round(mean(CEMA), 2), 
              median = round(median(CEMA),2), 
              sd = round(sd(CEMA), 2), 
              se = round(sd(CEMA)/sqrt(n),2))

## save estimates that we need for calculation of CI by hand down below 

cema_smoke_sd <- sum_tab$sd[1]
cema_nonsmoke_sd <- sum_tab$sd[2]

cema_smoke_n <- sum_tab$n[1]
cema_nonsmoke_n <- sum_tab$n[2]

colnames(sum_tab) <- c("Group", "N", "Mean CEMA","Median CEMA", "Standard Deviation", "Mean Standard Error")


sum_tab %>% 
  kbl(align = c(rep('c', 4)), booktabs = T) %>% 
  kable_styling(latex_options = c("striped","HOLD_position"), full_width = F, position = "center") %>% 
  column_spec(c(2:5), width = "1.5cm")
```

We can see that the average CEMA measurement is higher for a group of
Smokers. However, the difference in median values is even greater.
Standard deviation is quite high in these samples, while sample size
brings the standard error down. The two samples do not have equal variance, which violates one of t-test's assumptions. 

The proportional difference in mean and median values implies that the distribution
could be skewed. We can examine distribution shape before conducting the
test.

```{r, fig.width=9}
total <- 
  ggplot(data = e_cig, 
       aes(x = CEMA)) + geom_histogram(binwidth = 5, color = "black", fill = "white") + 
  theme_minimal() + 
  ylab("Count") + 
  ggtitle("Distribution of CEMA across two samples")
  

by_group <- 
  ggplot(data = e_cig, 
       aes(x = CEMA, 
           group = Group, 
           color = Group)) + geom_histogram(binwidth = 5, fill = "white") + 
  theme_minimal() + 
  ylab("Count") + 
  ggtitle("Distribution of CEMA Within Groups")

grid.arrange(total, by_group, nrow = 1)
```

While we can see that the distributions of two samples are heavily
skewed, and has a lot of outliers. We would want to transform this
distribution to the logarithmic scale to achieve 'normal shape', however, the t-test
is robust, so we can conduct it anyway.

We define $\overline X_1$ = mean CEMA for smokers and $\overline X_2$ =
mean CEMA for non-smokers.

Then, the null hypothesis is: $H_0: \overline X_1 = \overline X_2$ 

And
the alternative hypothesis is: $H_a: \overline X_1 \neq \overline X_2$

Results of T-test are given below:

```{r}

t_test_res <- 
  t.test(x = e_cig[e_cig$Group == "E-CIG", ]$CEMA,  # for t.test function we need to partition the samples manually 
       y = e_cig[e_cig$Group == "Non-smoker", ]$CEMA)

stat <- round(t_test_res$statistic, 2)

df <- t_test_res$parameter

p_val <- round(t_test_res$p.value, 4)

conf_int <- paste("(", round(t_test_res$conf.int[1], 2), ",", round(t_test_res$conf.int[2],2),  ")")

est_sm <- round(t_test_res$estimate[1], 2)
est_n_sm <- round(t_test_res$estimate[2], 2)

est_diff <- round(t_test_res$estimate[1] - t_test_res$estimate[2], 2)

```

Test results summary and interpretation:

-   The average CEMA levels for smokers were `r est_sm` and `r est_n_sm`
    for non-smokers.

-   Test statistic: `r stat` with `r df` degrees of freedom

-   Estimated difference between sample averages is `r est_diff`. 95%
    Confidence interval: `r conf_int`.

-   P-value: `r p_val`. P-value was greater than 0.05 and confidence
    interval included 0.

-   Concision: we do not have enough evidence to reject the null
    hypothesis. There is not enough evidence to suggest that the average
    levels of CEMA are difference between smokers and non-smokers.

### Test of average HOP1 levels

Again, we begin this section with the examination of data. Table below
shows all statistics of interest.

```{r}
sum_tab <- 
  e_cig %>% 
    group_by(Group) %>% 
    summarize(available = n() - sum(is.na(HOP1)),
              missing_values = sum(is.na(HOP1)), 
              mean = round(mean(HOP1, na.rm = T), 2), 
              median = round(median(HOP1, na.rm = T), 2), 
              sd = round(sd(HOP1, na.rm = T), 2), 
              se = round(sd(HOP1, na.rm = T)/
                           (n() - sum(is.na(HOP1))) ,2))

## save estimates that we need for calculation of CI by hand down below 

hop_smoke_sd <- sum_tab$sd[1]
hop_nonsmoke_sd <- sum_tab$sd[2]

hop_smoke_n <- sum_tab$available[1]
hop_nonsmoke_n <- sum_tab$available[2]

colnames(sum_tab) <- c("Group", "N for Analysis","N Missing", "Mean HOP1", "Median HOP1","Standard Deviation", "Mean Standard Error")

sum_tab %>% 
  kbl(align = c(rep('c', 4)), booktabs = T) %>% 
  kable_styling(latex_options = c("striped","HOLD_position"), full_width = F) %>% 
  column_spec(c(2:7), width = "1.5cm")
```

It appears that we have some missing values of HOP1 in the data, so we
present the number of data points available for analysis. Standard error
was estimated using the number of data point available for analysis.

We should not expect any statistically significant results here, the
average values are quite similar, while the median values are slightly
further apart. The shape of the distribution may be skewed in this case
too.

Once more, let's examine the shape and visual properties of distribution
before the test.

```{r, fig.width=9, warning=F, message=F}
total <- 
  ggplot(data = e_cig, 
       aes(x = HOP1)) + geom_histogram(binwidth = .05, color = "black", fill = "white") + 
  theme_minimal() + 
  ylab("Count") + 
  ggtitle("Distribution of HOP1 across two samples")
  

by_group <- 
  ggplot(data = e_cig, 
       aes(x = HOP1, 
           group = Group, 
           color = Group)) + geom_histogram(binwidth = .05, fill = "white") + 
  theme_minimal() + 
  ylab("Count") + 
  ggtitle("Distribution of HOP1 Within Groups")

grid.arrange(total, by_group, nrow = 1)
```

While we can see that the distributions of two samples are heavily
skewed, and has a lot of outliers. We would want to transform this
distribution to the logarithmic scale to achieve 'normal shape', however, the t-test
is robust, so we can conduct it anyway.

We define $\overline X_1$ = mean HOP1 for smokers and $\overline X_2$ =
mean HOP1 for non-smokers.

Then, the null hypothesis is: $H_0: \overline X_1 = \overline X_2$ 

And
the alternative hypothesis is: $H_a: \overline X_1 \neq \overline X_2$

Results of T-test are given below, due to the scale of these
measurements we round estimates to 5 decimal points:

```{r}

t_test_res <- 
  t.test(x = e_cig[e_cig$Group == "E-CIG"  & !is.na(e_cig$HOP1), ]$HOP1,  # for t.test function we need to partition the samples manually 
       y = e_cig[e_cig$Group == "Non-smoker" & !is.na(e_cig$HOP1), ]$HOP1)

stat <- round(t_test_res$statistic, 2)

df <- t_test_res$parameter

p_val <- round(t_test_res$p.value, 6)

conf_int <- paste("(", round(t_test_res$conf.int[1], 5), ",", round(t_test_res$conf.int[2],5),  ")")

est_sm <- round(t_test_res$estimate[1], 5)
est_n_sm <- round(t_test_res$estimate[2], 5)

est_diff <- round(t_test_res$estimate[1] - t_test_res$estimate[2], 5)

```

Test results summary and interpretation:

-   The average HOP1 levels for smokers were `r est_sm` and `r est_n_sm`
    for non-smokers.

-   Test statistic: `r stat` with `r df` degrees of freedom

-   Estimated difference between sample averages is `r est_diff`. 95%
    Confidence interval: `r conf_int`.

-   P-value: `r p_val`. P-value was greater than 0.05 and confidence
    interval included 0.

-   Concision: we do not have enough evidence to reject the null
    hypothesis. There is not enough evidence to suggest that the average
    levels of HOP1 are difference between smokers and non-smokers.

## (2) Wilcoxon Test.

### Test of median CEMA levels

As we saw in the previous section, the distribution of measurement is
skewed and has a lot of extreme values. Perhaps, it had an impact on the
t-test. I do not expect that we will see a meaningful change in result
for HOP1 comparison. Summary statistics suggest that the biomarker is
distributed almost identically for smokers and non-smokers. However, we
saw that the mean CEMA levels were actually quite different, so applying
a non-parametric test to these data can have an effect on our results.

Wilcoxon test allows us to test for difference in median values. We
begin our tests by comparing median values of CEMA levels.

We define $M(X_1)$ = median CEMA levels for smokers and $M(X_2)$ =
median CEMA levels for non-smokers. 

Therefore, the null hypothesis is
$H_0: M(X_1) = M(X_2)$

And the alternative hypothesis is $H_a: M(X_1) \neq M(X_2)$

Test results are given below:

```{r}
wil_res <- 
  wilcox.test(x = e_cig[e_cig$Group == "E-CIG", ]$CEMA, 
            y = e_cig[e_cig$Group == "Non-smoker", ]$CEMA, 
            
            conf.int = T,
            conf.level = .95, 
            est_diff = T)

p_val <- round(wil_res$p.value, 6)

stat <- round(wil_res$statistic,2)

smoke_med <- 
  round(median(e_cig[e_cig$Group == "E-CIG", ]$CEMA),2)
non_smoke_med <- 
  round(median(e_cig[e_cig$Group == "Non-smoker", ]$CEMA),2)

est_diff <- 
  round(wil_res$estimate,2)

conf_int <- 
  paste(
  "(", round(wil_res$conf.int[1],2), ",", round(wil_res$conf.int[2],2), ")" 
  ) 

```

Test results summary and interpretation:

-   Test statistic: `r stat`

-   Estimated median CEMA levels for non-smokers was `r non_smoke_med`
    and `r smoke_med` for smokers.

-   Estimated difference was `r est_diff` with a `r conf_int` 95%
    confidence interval

-   P-value was `r p_val`

-   Conclusion: the p-value was well below 0.05, and the confidence
    interval did not include 0. Therefore, we have enough statistical
    evidence to reject the null hypothesis and conclude that the median
    levels of CEMA for smokers are higher than median levels of
    non-smokers, by a magnitude of two.

Wilcoxon test allowed us to see that for these skewed distributions the
center of the distribution, and common values, were at a much higher
levels for smokers than non-smokers. Therefore, CEMA can be used as a
potentially useful predictor of smoking status, however, due to natural
variance of the data and presence of extreme values and a long tail, it
needs to be taken in context with other predictors and biomarkers.

### Test of median HOP1 levels

We define $M(X_1)$ = median HOP1 levels for smokers and $M(X_2)$ =
median HOP1 levels for non-smokers. 

Therefore, the null hypothesis is
$H_0: M(X_1) = M(X_2)$

And the alternative hypothesis is $H_a: M(X_1) \neq M(X_2)$

Test results are given below:

```{r}
wil_res <- 
  wilcox.test(x = e_cig[e_cig$Group == "E-CIG", ]$HOP1, 
            y = e_cig[e_cig$Group == "Non-smoker", ]$HOP1, 
            
            conf.int = T,
            conf.level = .95, 
            est_diff = T)

p_val <- round(wil_res$p.value , 6)

stat <- round(wil_res$statistic,2)

smoke_med <- 
  round(median(e_cig[e_cig$Group == "E-CIG", ]$HOP1),2)
non_smoke_med <- 
  round(median(e_cig[e_cig$Group == "Non-smoker", ]$HOP1),2)

est_diff <- 
  round(wil_res$estimate,2)

conf_int <- 
  paste(
  "(", round(wil_res$conf.int[1],2), ",", round(wil_res$conf.int[2],2), ")" 
  ) 

```

Test results summary and interpretation:

-   Test statistic: `r stat`

-   Estimated median CEMA levels for non-smokers was `r non_smoke_med`
    and `r smoke_med` for smokers.

-   Estimated difference was `r est_diff` with a `r conf_int` 95%
    confidence interval

-   P-value was `r p_val`

-   Conclusion: P-value was close to 0.05. The estimate was positive,
    and the confidence interval contained mostly positive values.
    However, the confidence interval did include 0. These results
    suggest that the median levels of HOP1 could be different between
    smokers and non-smokers. However, more data is needed to gather more
    evidence in order to reject the null hypothesis.

Overall, applying Wilcoxon tests to these two samples shows one case where non-parametric tests are more applicable than tests. 
He observe that we can extract more useful information using non-parametric tests when working with observational data. 
Such data are usually prone to variance that is outside of our control, and is different in groups that we wish to compare. 
Moreover, extreme values and outliers also make application of t-test difficult. 

## (3) Confidence intervals

In the previous section I have provided confidence intervals in the results interpretation section. In this section we validate 
those results by plugging estimates from the data into the formula. 

First, we will have confidence intervals for comparison of average
levels of CEMA

```{r, include = F}

t_test_res <- 
  t.test(x = e_cig[e_cig$Group == "E-CIG", ]$CEMA,  # for t.test function we need to partition the samples manually 
       y = e_cig[e_cig$Group == "Non-smoker", ]$CEMA)

stat <- round(t_test_res$statistic, 2)

df <- t_test_res$parameter

p_val <- round(t_test_res$p.value, 4)

conf_int <- paste("(", round(t_test_res$conf.int[1], 2), ",", round(t_test_res$conf.int[2],2),  ")")

est_sm <- round(t_test_res$estimate[1], 2)
est_n_sm <- round(t_test_res$estimate[2], 2)

est_diff <- round(t_test_res$estimate[1] - t_test_res$estimate[2], 2)

```

We need samples sizes, standard deviations of the two samples, and a coefficient for our desired level of confidence, which is 1.96.

The formula is given below: 

Standard Error of difference = 
$$\large \sqrt{\frac{SD_{smokers}^2}{N_{smokers}} + \frac{SD_{non-smokers}^2}{N_{non-smokers}}}$$

```{r}
std_err_diff <- sqrt( (cema_smoke_sd^2/cema_smoke_n + cema_nonsmoke_sd^2/cema_nonsmoke_n) )
```

Resulting standard error for the difference estimate is `r std_err_diff`

The upper bound is `r est_diff` + 1.96 * `r std_err_diff` = `r est_diff + 1.96 * std_err_diff`

The lower bound is `r est_diff` - 1.96 * `r std_err_diff` = `r est_diff - 1.96 * std_err_diff`


Then we need to weight this result by 

-   Estimated difference between sample averages is `r est_diff`. 95%
    Confidence interval: `r conf_int`.

This supports our results obtained in the first section. We saw the
estimate for difference in means was not statistically significant,
because the p-value was above 0.05. Confidence intervals includes 0,
which is another piece of evidence in favor of null hypothesis.

Now we examine confidence interval for HOP1 difference

```{r, include = F}

t_test_res <- 
  t.test(x = e_cig[e_cig$Group == "E-CIG", ]$HOP1,  # for t.test function we need to partition the samples manually 
       y = e_cig[e_cig$Group == "Non-smoker", ]$HOP1)

stat <- round(t_test_res$statistic, 2)

df <- t_test_res$parameter

p_val <- round(t_test_res$p.value, 4)

conf_int <- paste("(", round(t_test_res$conf.int[1], 2), ",", round(t_test_res$conf.int[2],2),  ")")

est_sm <- round(t_test_res$estimate[1], 2)
est_n_sm <- round(t_test_res$estimate[2], 2)

est_diff <- round(t_test_res$estimate[1] - t_test_res$estimate[2], 2)

```

Similarly, we calculate confidence interval using values from the data 

```{r}
std_err_diff <- sqrt( (hop_smoke_sd^2/hop_smoke_n + hop_nonsmoke_sd^2/hop_nonsmoke_n) )
```

Resulting standard error for the difference estimate is `r std_err_diff`

The upper bound is `r est_diff` + 1.96 * `r std_err_diff` = `r est_diff + 1.96 * std_err_diff`

The lower bound is `r est_diff` - 1.96 * `r std_err_diff` = `r est_diff - 1.96 * std_err_diff`

-   Estimated difference between sample averages is `r est_diff`. 95%
    Confidence interval: `r conf_int`. Note that there are minor discrepancies due to rounding.  

This supports our results obtained in the first section. We saw the
estimate for difference in means was not statistically significant,
because the p-value was above 0.05. Confidence intervals includes 0,
which is another piece of evidence in favor of null hypothesis.

# 3.1

```{r, include= F}
infant <- read_xls("/Users/denisostroushko/Desktop/UofM MS/MS Fall 2022/Puhb 7405/Data Sets/Infants.xls")

original_names <- colnames(infant)

colnames(infant) <- c("head_c", "length", "gest_weeks", "birthwght", "mom_age", "toxemia")

```

### (1) Scatterplot

```{r, warning=F, message= F}

ggplot(data = infant, 
       aes(x = gest_weeks, 
           y = birthwght)) + geom_point() + 
  stat_smooth(aes(color = "LOESS Smooth Trend Line")) + 
  stat_smooth(se= F, method = "lm", aes(color = "Fitted Line")) + 
  
  theme_minimal() + 
  xlab("Number of Gestional Weeks") + 
  ylab("Infant Birth Weight") + 
  
  ggtitle(
    paste("Correlation Between Gestational Weeks and \
                Infant Birth Weight: correlation coefficient ", 
                round(
                  cor(
                    infant$gest_weeks, infant$birthwght
                      )
                  ,3)
                )
          )

```

The relationship between gestational weeks and birth weight of infant have a linear relationship. Using LOESS smooth averaged trend line 
(in teal) we look for any non-linear curvature in the data. However, it is clear that LOESS smooth line is very close to a fitted trend line 
(in red). Therefore, we have visual conformation that the relationship is indeed linear. 

We can see that the data is distributed in a fairly narrow cluster of point around the ends of the fitted line, and with more variance 
around the middle of the cluster. This is a very typical distribution of two continuous variables. 

### (2) Test of independence 

We need to define hypothesis for any test that we perform. So, 

the null hypothesis assumes no correlation between variables and is given by $H_0: r = 0$

the alternative hypothesis is given by $H_a: r \neq 0$

The results of test for independence are given below: 

```{r}
cor_res <- cor.test(infant$gest_weeks, infant$birthwght)

stat <- round(cor_res$statistic, 4)

df <- cor_res$parameter

p_val <- round(cor_res$p.value, 6)

cor_est <- round(cor_res$estimate, 4)

cor_ci <- 
  paste(
    "(", round(cor_res$conf.int[1], 4), ",", round(cor_res$conf.int[2], 4),")"
  )

```

Test results: 

* Test statistic is `r stat`

* Estimated Pearson's correlation coefficient is `r cor_est` with a `r cor_ci` 95% confidence interval

* P-value is `r p_val` on `r df` degrees of freedom

* Conclusion: p-value is essentially 0 and the confidence interval does not include 0. Therefore, we have enough statistical evidence 
to reject null hypothesis and conclude that the number of gestational weeks and infant's birth weight are statistically positively 
correlated. 

## (3) Confidence Interval for Test of Independence

In this section we calculate a confidence interval for Pearson's correlation coefficient using data and formulas. 

Step one is to obtain a coefficient from Fischer Transformation

```{r}
z = 1/2 * log((1+cor_est)/(1-cor_est) )
```

z transformation = $\frac{1}{2} \times ln(\frac{1+correlation estimate}{1 - correlation estimate})$ = `r z`

Step two is to obtain standard error 

```{r}
se_z = sqrt(1/(nrow(infant) - 3))
```

standard error = $\sqrt{\frac{1}{N - 3}}$ = `r se_z`

So, the raw upper bound is `r z` + 1.96 * `r se_z` = `r z + 1.96 * se_z`

So, the raw lower bound is `r z` - 1.96 * `r se_z` = `r z - 1.96 * se_z`

The last step is to transform back from Fischer Z back to the original scale

original scale lower or upper bound = $\frac{e^{2 * bound} - 1}{e^{2 * bound} + 1}$, which results in 
(`r (exp(2 * (z - 1.96 * se_z))-1)/(exp(2 * (z - 1.96 * se_z))+1)`, 
  `r (exp(2 * (z + 1.96 * se_z))-1)/(exp(2 * (z + 1.96 * se_z))+1)`)

Confidence interval is `r cor_ci`, it does not include 0 and gives more evidence in favor of rejection of null hypothesis

### (4) Spearman's rho correlation coefficient

We had to use two separate functions to both do the Spearman's $\rho$ test and obtain a confidence interval for it. 

We keep the same null and alternative hypotheses, just this time we use a different method to calculate a correlation coefficient 
and perform test of independence. 

Test results are below: 

```{r}

spe_cor <- cor.test(infant$gest_weeks, infant$birthwght, method = "spearman")

spe_cor2 <- SpearmanRho(x = infant$gest_weeks, y = infant$birthwght, conf.level = .95)


stat <- round(spe_cor$statistic, 4)

spe_est <- round(spe_cor$estimate, 4)

spe_confint <- 
  paste(
    "(", round(spe_cor2[2], 4), ",", round(spe_cor2[3], 4), ")"
  )


p_val <- round(spe_cor$p.value, 4)


```

* Test statistic is `r stat`

* Estimated Pearson's correlation coefficient is `r spe_est` with a `r spe_confint` %95 confidence interval

* P-value is `r p_val` 

* Conclusion: p-value is essentially 0 and the confidence interval does not include 0. Therefore, we have enough statistical evidence 
to reject null hypothesis and conclude that the number of gestational weeks and infant's birth weight are statistically positively 
correlated. 

The difference between Pearson's coefficient and Spearman's coefficient is `r round(cor_est - spe_est, 4)`, rounded to 4 decimal points.

The data did not have any notable problems with outliers or non-linear patterns, and I presume that these are the reasons why 
parametric and non-parametric methods produce the same results. We can also see that the lower and upper bounds of confidence intervals are 
similar for both methods, which gives us more confidence in obtained results. 

---
title: "Homework 6"
author: "Denis Ostroushko"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    extra_dependencies: ["float"]
editor_options: 
  markdown: 
    wrap: 72
---

```{r, echo = F}
knitr::opts_chunk$set(echo = T, message = F, warning = F, fig.pos = "!H")
options(scipen=999)
```

```{r, include=F}
library(MASS)
require(tidyverse) # require instead of library to make sure that other packages do not overwrite tidyverse packages 
library(kableExtra)
library(readxl)
library(gridExtra)
library(ggeffects)
library(mltools) # one hot encoding outside of caret package 
library(data.table) # need this for mltools to work 
library(olsrr) # a better package for stepwise regression 
```

```{r, eval=F}
library(MASS)
require(tidyverse) # require instead of library to make sure that other packages do not overwrite tidyverse packages 
library(kableExtra)
library(readxl)
library(gridExtra)
library(ggeffects)
library(mltools) # one hot encoding outside of caret package 
library(data.table) # need this for mltools to work 
library(olsrr) # a better package for stepwise regression 
```

# 12.2 

```{r, echo=F}
infants <- readxl::read_xls('/Users/denisostroushko/Desktop/UofM MS/MS Fall 2022/Puhb 7405/Data Sets/Infants.xls')
```

```{r}
colnames(infants) <- c("head_c", "length", "gest_weeks", "birth_w", "m_age", "toxemia")

# process the data and keep variables for analysis 

infants_f <- infants %>% 
  select(birth_w, gest_weeks, m_age)

```

## 12.2 - A

### Model Specifications and T-tests

Before fitting the model, we wish to investigate the relationship between mother's age and infant's
birth weight. Since the problem asks us to fit the model with age squared, we will have a second order polynomial relationship. We will look at the scatter plot to find any visual evidence that such 
model is justifiable. 

```{r, echo = F}
ggplot(data = infants_f, 
       aes(x = m_age, 
           y = birth_w)) + geom_point() + 
  
  stat_smooth(se = T, aes(color = "Smooth Trend Line")) + 
  stat_smooth(se = F, method = "lm", aes(color = "Regression Line")) + 
  
  scale_color_manual(values = c("Smooth Trend Line" = "blue", 
                                "Regression Line" = "red")) + 
  xlab("Mother's Age") + 
  ylab("Infant's Birth Weight") + 
  ggtitle(paste("Correlation Between Mother's Age  \n and Infant's Birth Weight: ", 
                round(cor(infants_f$m_age, 
                          infants_f$birth_w), 4))) + 
  theme_minimal()

```

We can see that we should fit the polynomial regression model because the smooth line shows a 
curved relationship between the two variables. However, the confidence bound around the smooth line 
suggest that potentially we may be able to fit a straight, first order, line in order to predict 
infant's birth weight. Overall, it is not very clear to what the verdict is, so we will fit the 
model with a higher order term and will use statistical tests to verify contribution of the squared 
term. Pearson's linear correlation estimate is low, so we should not expect to see string statistical 
evidence that mother's age is a strong predictor for infant's birth weight. 

It is known that inclusion of higher order terms introduces multicollinearity issue to the model, which is hard to handle, and affects confidence intervals for predictors. Normally, we wish to 
perform another transformation of variables called *centering* in order to reduce the degree of 
linear correlation between the linear and higher order terms, however, I decided to include that 
into the appendix. 

The plot below shows correlation between age and age squared. 

```{r, echo = F}
# look at the correlation between age and age^2 
ggplot(data = infants_f, 
       aes(x = m_age, 
           y = m_age^2 )) + geom_point() + 
  
  stat_smooth(method = "lm", se = F, color = "red") +
  
  xlab("Age") + 
  ylab("Age Squared") + 
  ggtitle(paste("Correlation: ", round(cor(infants_f$m_age, infants_f$m_age^2),4))) + 
  theme_minimal()

```

Since the two variables are almost perfectly correlated, we expect that estimate for the standard 
error of $\hat \beta_i$ are higher in the model with no centering transformation applied. We verify
it in the appendix section. 

We are now ready to fit the model and explore the contribution of age-squared term. Model 
specification: 

$$\Large E[Y] = \hat \beta_0 + \hat \beta_1 * Gestional \ Weeks + \hat \beta_2 * Mother's \ Age + \hat \beta_3 * Mother's \ Age ^ 2$$

We obtain model the estimates from the model and present them in the table below: 

```{r, echo = F}
inf_lm <- lm(birth_w ~ gest_weeks + m_age + I(m_age^2), data = infants_f)

sum_data <- data.frame(summary(inf_lm)$coefficients)

sum_data$names <- c("Intercept", "Gestational Weeks", "Mother's Age", "Mother's Age Squared")

rownames(sum_data) <- NULL

sum_data <- sum_data %>%  dplyr::select(names, everything())

round_3 <- function(x){round(x,3)}
sum_data[,2:5] <- lapply(sum_data[,2:5], round_3)

colnames(sum_data) <-c("Model Term", "Estimate", "Std. Error", "T-value", "P-value") 

sum_data %>% 
  kbl(booktabs = T, align = c('l', 'c','c','c','c'), 
      caption = "Polymonial Regression Estimates") %>% 
  kable_styling(latex_options = c("striped", "HOLD_position"))

```

Comments: 

* R-squared is `r round(summary(inf_lm)$r.squared,4)`  and Adjusted R-squared is `r round(summary(inf_lm)$adj.r.squared,4) ` 

* The number of gestational weeks is an extremely strong predictor of the infant's birth weight. 
  Each additional week add an average of `r sum_data[2,2]` units of measurement (not sure what they 
  are in this problem) to infant's birth weight 

* Both linear are quadratic terms for mother's age are not statistically significant, and therefore 
  we do not have enough evidence to reject the null hypothesis and conclude that the coefficients 
  for these predictors are statistically different from zero. 
  
* An addition of a quadratic term turns the effect of age on birth weight from a straight line to the 
  parabola. We can use estimates of the linear and quadratic terms to describe the shape of this 
  parabola. 
  
  + A positive quadratic coefficient causes the ends of the parabola to point upward. A negative   
      quadratic coefficient causes the ends of the parabola to point downward. The greater the       
      quadratic coefficient, the narrower the parabola. The lesser the quadratic coefficient, the    
      wider the parabola. 
      
  + In our case the coefficient is `r sum_data[4,2]`, so the effect can 
      be visualized as a wide downward-pointing parabola. 
      
  + A very wide parabola usually does not indicate a strong effect, and visually it should appear 
    closer to a straight line with a zero linear coefficient. 

### Evaluate Extra Sum of Squares 

We already know that the linear and quadratic terms of mother's age are not strong predictors of 
birth weight. In this section we will specifically investigate the extra sum of squares and 
partial coefficient of determination to describe their predictive power in more depth. 

I used built in `R` functions to create an ANOVA table to decompose SSR and obtain partial 
coefficients of determination. 

```{r, echo = F}
anv <- data.frame(anova(inf_lm) )

anv$names <- c("Gestational Weeks", "Mother's Age", "Mother's Age Squared", "Residuals")
rownames(anv) <- NULL
anv <- anv %>% select(names, everything())

colnames(anv) <- c("Model Term", "DF", "SS", "MS", "F-statistic", "P(F* > F)")

anv[,length(anv)] <- round(anv[,length(anv)], 4)

anv %>% 
  kbl(booktabs = T, align = c('l', rep('c', length(anv)-1)) ) %>% 
  kable_styling(latex_options = c("striped", "HOLD_position")) %>% 
  row_spec(nrow(anv)-1, hline_after = T)

```

```{r, echo = F}
# R^2 (m age | gest weeks )
lm_2 <- lm(birth_w ~ gest_weeks + m_age, data = infants_f)
ssr_2 <- sum( (lm_2$fitted.values - mean(infants_f$birth_w) )^2 )

# R^2 (m age ^2  | m age, gest weeks )
lm_3 <- lm(birth_w ~ gest_weeks + m_age + I(m_age^2), data = infants_f)

sse_2 <- sum(lm_2$residuals^2)
ssr_3 <- sum( (lm_3$fitted.values - mean(infants_f$birth_w) )^2 )

Rs_3 <- (ssr_3 - ssr_2) /sse_2

```

* Extra SSR for mother's age squared is: `r  anv[3,4]`

* Extra $R^2$ of mother's age squared after the addition of gestational weeks and linear term for 
  mother's age is: `r round(Rs_3, 4)`, which is low

* We can see once again that these F tests are directly related to the T test we obtain from the model
  summary. The connection can be seen by observing that the p-values are the same, and F-test 
  statistics is the square of the T-test statistic. 
  
* The p-value of Extra SSR for the squared mother's age term is `r anv[3,6]`, so we can't reject the 
  null hypothesis. Therefore, there is no statistical evidence that the squared mother's age 
  term helps to meaningfully explain variation in birth weight. 
  
* Both linear and quadratic terms for mother's age can be removed. 

### Visualize Model Effects

We conclude this section by visualizing the effects both linear and quadratic terms on the birth weight of an infant. As we can see, the fitted effect is a downward-facing parabola, as we 
expected by looking at the coefficients. 

```{r, echo = F }
mydf <- ggpredict(inf_lm, terms = "m_age")

mean_name <- paste0("Birth Weight Mean Value: ", round(mean(infants_f$birth_w)))

ggplot(mydf, aes(x, predicted)) +
  geom_line() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .1) + 
  theme_minimal() + 
  geom_hline( 
             aes(yintercept = mean(infants_f$birth_w),color = mean_name)
             ) + 
  guides(color=guide_legend(title="Additional Elements: ")) + 
  theme(
    legend.position = "bottom"
  ) + 
  
  ylab("Estimated Effect") + 
  xlab("Mother's Age") + 
  ggtitle("Model Estiamted Effects of Mother's Age on \n Infant's Birth Weight")
  

```

This plot visually confirms all inferences and conclusions we have made thus far about the 
linear and quadratic terms of mother's are. 

* We can see that the confidence bound around fitted effect line includes a flat straight line 
  with the zero-coefficient. So, a 95% confidence interval includes a 'scenario' where the 
  predictors show no effect on the response variables 
  
* We have a very wide parabola with a downward-facing ends, which is the result of a very small 
  absolute values of a coefficient and a negative value of a coefficient. 

### Interpretation of Mother's Age Coefficients 

It does not make sense to interpret the linear term while "holding the square term constant", 
because the two are a function of the same measurement. This conclusion also follows from the fact 
that the two variables are correlated both mathematically and physically. 

## 12.2 - B

Correlation Transformation for variables $Y, X_1, ..., X_{p-1}$, denoted by $V$, is given by this 
formula: 

$$\Large V^* = \frac{1}{\sqrt{n - 1}} \times \Big (\frac{V - \bar V}{sd(V)} \Big )$$
We use the code below to transform the variables

```{r}

correlation_transformation <- 
  function(X, n = nrow(infants_f_cor_tr)){
    
    1/(sqrt(n - 1)) * (X - mean(X))/sd(X)
    
  }

infants_f$m_age_sq <- infants_f$m_age^2
infants_f_cor_tr <- infants_f

infants_f_cor_tr <- data.frame(lapply(infants_f_cor_tr, correlation_transformation))
```

Using transformed variables, we use this model specification and provide a summary for this model 
below: 

$$\Large E[Y^*] = \hat \beta_0 + \hat \beta_1 * Gestional \ Weeks^* + \hat \beta_2 * Mother's \ Age^* + \hat \beta_3 * (Mother's \ Age^*)^2$$


In addition to the correlation transformed coefficient, we provide a summary table for the model of 
unstransformed variables, which we saw in part (A)

```{r, echo = F}
cor_tr_lm <- lm(birth_w ~ . , data = infants_f_cor_tr)
sum2 <- data.frame(summary(cor_tr_lm)$coefficients)

sum2$names <- c("Intercept", "Gestational Weeks", "Mother's Age", "Mother's Age Squared")

rownames(sum2) <- NULL

sum2 <- sum2 %>%  dplyr::select(names, everything())

round_3 <- function(x){round(x,3)}
sum2[,2:5] <- lapply(sum2[,2:5], round_3)

colnames(sum2) <-c("Model Term", "Estimate", "Std. Error", "T-value", "P-value") 

kbl(sum_data, booktabs = T, caption = "Original Scale Regression Estimates") %>% 
  kable_styling(latex_options = c("striped", "HOLD_position"))

kbl(sum2, booktabs = T, caption = "Correlation  Transformation Regression Estimates") %>% 
  kable_styling(latex_options = c("striped", "HOLD_position"))

```

Comments: 

* Intercept is zero when we fit the model to the correlation transformed variables, which is to be 
  expected. 

* T and P values are the same for the covariates in both models, which suggests that the scale of 
  the predictors should not affect the statistical tests and results for the inference of coefficients
  and standard errors. 

* Therefore, same conclusions apply for the correlation transformed regression model and original scale
  model. 
  
## 12.2 - C

Transformation back to the original scale is given by this formula: 

For variables $X_1, ..., X_{p-1}$: 

$$\Large \hat \beta_i =  \hat \beta^*_i \times \frac{sd(Y)}{sd(X_i)}$$
Once we obtain the original scale estimates, we can calculate the intercept term. We omit this 
calculation here. If we verify that $\hat \beta_1 , ..., \hat \beta_{p-1}$ match the original estimates
after the transformation back, we know that we also will obtain the proper intercept term. 

Back transformation will be done using code in the chunk below: 

```{r}
transform_back <-
  function(Beta_star, s_x, s_y){
    Beta_star * (s_y / s_x)
  }

S_Y <- sd(infants_f$birth_w)
```

I hid the code that constructs a data frame for presentation. In addition to the $\hat \beta_i$ 
estimates I included their confidence intervals. It was of interest to me to verify that linear 
transformation applies to both the coefficient and the standard errors. 

```{r, echo = F}
check_res <- 
  data.frame(
    names = c("Gestation Weeks", "Mother's Age", "Mother's Age Squared"), 
    betas = c(transform_back(sum2[2,2], s_x = sd(infants_f$gest_weeks), s_y = S_Y),
              transform_back(sum2[3,2], s_x = sd(infants_f$m_age), s_y = S_Y),
              transform_back(sum2[4,2], s_x = sd(infants_f$m_age_sq), s_y = S_Y)
              ), 
    ci_l = c(transform_back(confint(cor_tr_lm)[2,1], s_x = sd(infants_f$gest_weeks), s_y = S_Y),
              transform_back(confint(cor_tr_lm)[3,1], s_x = sd(infants_f$m_age), s_y = S_Y),
              transform_back(confint(cor_tr_lm)[4,1], s_x = sd(infants_f$m_age_sq), s_y = S_Y)
              ),
    ci_h = c(transform_back(confint(cor_tr_lm)[2,2], s_x = sd(infants_f$gest_weeks), s_y = S_Y),
              transform_back(confint(cor_tr_lm)[3,2], s_x = sd(infants_f$m_age), s_y = S_Y),
              transform_back(confint(cor_tr_lm)[4,2], s_x = sd(infants_f$m_age_sq), s_y = S_Y)
              )
  )

```


Recall the the original model with the transformed variables was called `inf_lm`. 
I used it for Extra SS, t-tests and model effects in the previous sections. 

```{r}
conf <- data.frame(confint(inf_lm)) # just the confidence intervals 
conf <- cbind(coefficients(inf_lm), conf )
```

We can obtain standard errors and confidence intervals for the estimates
to compare with the transformation back from the correlation transformation procedure. 

```{r, echo=F}

conf$names <- c("Intercept", "Gestation Weeks", "Mother's Age", "Mother's Age Squared")
rownames(conf) <- NULL

conf <- conf %>% select(names, everything())

colnames(conf) <- c("Model Term", "Coefficient", "95% C.I. Lower Bound", "95% C.I. Upper Bound")

round_3 <- function(x){round(x,3)}
conf[,2:4] <- sapply(conf[, 2:4], round_3)


colnames(check_res) <- c("Model Term", "Coefficient", "95% C.I. Lower Bound", "95% C.I. Upper Bound")

check_res[,2:4] <- lapply(check_res[,2:4], round_3)


conf %>% filter(`Model Term` != "Intercept" ) %>% 
  kbl(align = c('l', rep('c', length(conf)-1)), booktabs =  T, 
      caption = "Original Model Estiamtes and C.I.") %>% 
  kable_styling(latex_options = c("striped", "HOLD_position")) 

check_res %>% filter(`Model Term` != "Intercept" ) %>% 
  kbl(align = c('l', rep('c', length(conf)-1)), booktabs =  T, 
      caption = "Estimaes and C.I. obtained via \n Back-Trnasformation") %>% 
  kable_styling(latex_options = c('striped', "HOLD_position")) 
```

As we can see, the results matched. 

\newpage 

# 13.4 

```{r, echo = F}
cig <- read_xlsx('/Users/denisostroushko/Desktop/UofM MS/MS Fall 2022/Puhb 7405/Data Sets/E-CID-3.xlsx')
```

```{R}
cig$Y1 <- with(cig, log(NNAL_vt4_creat / NNAL_vt0_creat))
cig$Y2 <- with(cig, log(TNE_vt4_creat / TNE_vt0_creat))

cig <- cig %>% 
  select(Y1, Y2, arm, age, gender, white, educ2, income30, FTND)

colnames(cig)[length(cig)] <- "ftnd"
```

## 13.4 - A

We can summarize each variable that we consider for analysis and use this table to find the 
number of predictors that we will have: 

```{r}

cig <- cig %>% select(
  Y1, Y2, age, arm, gender, educ2, income30, ftnd
)

cig$arm <- as.factor(cig$arm)

cig <- data.frame(one_hot(as.data.table(cig))) %>% select(-arm_5)

cig[,4:(length(cig)-1)] <- lapply(cig[,4:(length(cig)-1)], as.factor)
```


```{r}
n_unique <- function(x){length(unique(x))}

meta_data <- 
  
  data.frame(
    class = sapply(cig, class), 
    n_unique = sapply(cig, n_unique)
  )
```

```{r, echo = F}

meta_data$name <- rownames(meta_data)

rownames(meta_data) <- NULL

meta_data <- meta_data %>% filter(!(name %in% c("Y1", "Y2")))

meta_data <- 
  meta_data %>% select(name, everything())
  
colnames(meta_data) <- c("Predictors", "Assigned Class", "N of Unique Values")

meta_data %>% 
  kbl(caption = "Sumamry of Covariates",
       booktabs = T, align = c("l", 'c', 'c')) %>% 
  kable_styling(latex_options = c("striped", "HOLD_position"))

n_predictors <- nrow(meta_data)
```

After consideration of all variables that we need for analysis, we know that this is the final 
set of covariates. 

* Arm will result in $4 -1 = 3$ variables 

* Age is untouched 

* FTND is treated as continuous 

So, the total number of predictors is 3 for Arm indicators, plus other covariates, which results in 
`r n_predictors` total predictors. 

## 13.4 - B

In this section we will create a regression model for $Y1$, provide a summary table for model 
estimates, and look at three different ways to adjust p-values for multiple comparisons. 

We then will repeat this process for $Y2$. 

### Regression on Y1 

First, we specify the model in the code chunk below. Regression model has 8 predictors, so 
we will avoid writing the entire expression for $E[Y]$. 

```{r}
y1_lm1 <- lm(Y1 ~ . - Y2, data = cig )
```

Summary of coefficients and tests statistics is given below: 

```{r , echo = F}
sum2 <- data.frame(summary(y1_lm1)$coefficients)

sum2$names <- c("Intercept", "Age", "Arm 6", "Arm 7", "Arm 8", 
                "Gender", "Education", "Income >= $30K", "FTND")

rownames(sum2) <- NULL

sum2 <- sum2 %>%  dplyr::select(names, everything())

round_3 <- function(x){round(x,3)}
sum2[,2:5] <- lapply(sum2[,2:5], round_3)

colnames(sum2) <-c("Model Term", "Estimate", "Std. Error", "T-value", "P-value") 

kbl(sum2, booktabs = T, caption = "Original Scale Regression Estimates") %>% 
  kable_styling(latex_options = c("striped", "HOLD_position"))
```

```{r, echo = F}
p_vals <- data.frame(summary(y1_lm1)$coefficients)[2:nrow(data.frame(summary(y1_lm1)$coefficients)),4]

# c("holm", "hochberg", "hommel", "bonferroni", "BH", "BY",
#   "fdr", "none")

```

* It appears that at the $\alpha = 0.05$ significance level, participants from group 6 and 
  higher levels of income saw their measurements consistently reduced, after adjusting for other 
  predictors. Other variables do not show a statistically significant relationship with 
  the response variable. 
  
Since we have 8 tests for each predictor variable, we have $1 - (1 - 0.05)^8 =$ `r round(1- (1-0.05)^8, 4)` probability of 
at least one test being a false positive. This is not an incredibly high probability, but it 
definitely raises a cause for concern. Therefore, we need to make an adjustment for performing 
multiple comparisons at the same time. 

We begin with the **Bonferroni Adjustment**. This is the simplest, and most conservative adjustment. 
Given our desired significance level, $\alpha = 0.05$, the new level at which we declare significance
is $Bonferroni \ adjusted \ P-value =$ `r round(0.05 / n_predictors, 4)`. 

Thus, we present the table with predictors and corresponding p-values from the multiple regression 
model, and denote predictors which are statistically significant at the new level with a "*". 

```{r}
sum_bonf_adj <- sum2 %>% select(`Model Term`, `P-value`)
sum_bonf_adj$`Significant at Adj. Level` = 
  with(sum_bonf_adj, 
       ifelse(`P-value` < 0.05 / n_predictors , "*", "")
       )

sum_bonf_adj %>% 
  kbl( booktabs = T, caption = "Regression of Y1 Bonferroni Adjusted Comparison") %>% 
    kable_styling(latex_options = c("striped", "HOLD_position")) %>% 
    column_spec(3, width = "2cm")

```

it appears that still only an indicator for Arm 6 group is a statistically significant predictor. 

Next we would like to implement **HOLM Adjustments**. The procedure involves iterative evaluation 
of p-values using these steps: 

* order p-values smallest to largest 
    + if first p-value if smaller than $0.05 / 8 =$ `r round(0.05 / n_predictors, 4)` then 
      conclude significance, and move to next predictor, otherwise stop, none are significant 
      
* next predictor will be tested at $0.05 / 7 =$ `r round(0.05 / (n_predictors-1), 4)`. 
    + So, each time we declare a predictor significant, we decrease the denominator for the new 
      cutoff by 1. 
    
* The process stops when we find the first predictor that is not statistically significant 
    
We use the code below to evaluate p-values according to the adjustments and present the table with 
the results. Note that the predictors are sorted in the table from the smallest to the highest
original  p-value. 

```{r}

holm_data <- 
  sum2 %>% select(`Model Term`, `P-value`) %>% arrange(`P-value`) %>% 
  filter(`Model Term` != "Intercept")
  
holm_data$`Comparison P-value` <- 1
holm_data$`Significant at Adj. Level` <- ""

cur_adj_n <- n_predictors

for(i in 1:nrow(holm_data)){
  
  cur_level <- 0.05 / cur_adj_n
  holm_data[i,3] <- cur_level
  
  if(holm_data[i,2] <= cur_level ){
    cur_adj_n <- cur_adj_n - 1
    holm_data[i,3] <- cur_level
    holm_data[i,4] <- "*"
  }
}

holm_data[,2:3] <- lapply(holm_data[,2:3], round_3)

holm_data %>% 
  kbl( booktabs = T, caption = "Regression of Y1 HOLM Adjusted Comparison") %>% 
    kable_styling(latex_options = c("striped", "HOLD_position")) %>% 
    column_spec(c(3,4), width = "2cm")

```

Once again, Arm 6 indicator variable is the only statistically significant predictor. 

And finally we also implement the **Hochberg Adjustments**. The process is somewhat similar with 
the HOLM adjustment, but it is different enough that we should implement it. The process is given by:

* Sort P-values largest to smallest 
* Compare the largest to $0.05$, if significant, declare all significant
* Otherwise, compare the next one to $0.05/2 = 0.025$
* Keep comparing to $0.05/3$, $0.05/4$, etc.. until we find a comparison where the predictor is 
  not statistically significant, the first such predictor terminates the process. 

    
```{r}

hoch_data <- 
  sum2 %>% select(`Model Term`, `P-value`) %>% arrange(-`P-value`) %>% 
  filter(`Model Term` != "Intercept")

hoch_data$`Comparison P-value` <- 0.05
hoch_data$`Significant at Adj. Level` <- ""

cur_adj_n <- 1

for(i in 1:nrow(hoch_data)){
  
  cur_level <- 0.05 / cur_adj_n
  hoch_data[i,3] <- cur_level
 
  if(hoch_data[i,2] > cur_level){
    cur_adj_n <- cur_adj_n + 1
    
    hoch_data[i,3] <- cur_level
  }
}

hoch_data[,4] <- ifelse(hoch_data[,2] < hoch_data[,3], "*", "")

hoch_data[,2:3] <- lapply(hoch_data[,2:3], round_3)

hoch_data %>% 
  kbl( booktabs = T, caption = "Regression of Y1 HOCHBERG Adjusted Comparison") %>% 
    kable_styling(latex_options = c("striped", "HOLD_position")) %>% 
    column_spec(c(3,4),  width = "2cm")

```

Once again, Arm 6 indicator variable is the only statistically significant predictor. 

Therefore, after a series of p-value adjustments we can conclude that we do not have any false 
positive statistically significant predictors, and we can state that Arm 6 is the only statistically significant predictor after adjusting for other covaraites. 

### Regression on Y2 

Similarly, we will create the same set of summary and adjustment table for the regression model 
for Y2 variable, but will omit most of the commentary due to similarities with the Y1 regression 
model. 

```{r}
y2_lm1 <- lm(Y2 ~ . - Y1, data = cig )
```

```{r , echo = F}
sum2 <- data.frame(summary(y2_lm1)$coefficients)

sum2$names <- c("Intercept", "Age", "Arm 6", "Arm 7", "Arm 8", 
                "Gender", "Education", "Income >= $30K", "FTND")

rownames(sum2) <- NULL

sum2 <- sum2 %>%  dplyr::select(names, everything())

round_3 <- function(x){round(x,3)}
sum2[,2:5] <- lapply(sum2[,2:5], round_3)

colnames(sum2) <-c("Model Term", "Estimate", "Std. Error", "T-value", "P-value") 

kbl(sum2, booktabs = T, caption = "Original Scale Regression Estimates") %>% 
  kable_styling(latex_options = c("striped", "HOLD_position"))
```

* Bonferroni Adjustments 

```{r, echo = F}
sum_bonf_adj <- sum2 %>% select(`Model Term`, `P-value`)
sum_bonf_adj$`Significant at Adj. Level` = 
  with(sum_bonf_adj, 
       ifelse(`P-value` < 0.05 / n_predictors , "*", "")
       )

sum_bonf_adj %>% 
  kbl( booktabs = T, caption = "Regression of Y2 Bonferroni Adjusted Comparison") %>% 
    kable_styling(latex_options = c("striped", "HOLD_position")) %>% 
    column_spec(3, width = "2cm")

```

* HOLM Adjustments 

```{r, echo = F}

holm_data <- 
  sum2 %>% select(`Model Term`, `P-value`) %>% arrange(`P-value`) %>% 
  filter(`Model Term` != "Intercept")
  
holm_data$`Comparison P-value` <- 1
holm_data$`Significant at Adj. Level` <- ""

cur_adj_n <- n_predictors

for(i in 1:nrow(holm_data)){
  
  cur_level <- 0.05 / cur_adj_n
  holm_data[i,3] <- cur_level
  
  if(holm_data[i,2] <= cur_level ){
    cur_adj_n <- cur_adj_n - 1
    holm_data[i,3] <- cur_level
    holm_data[i,4] <- "*"
  }
}

holm_data[,2:3] <- lapply(holm_data[,2:3], round_3)

holm_data %>% 
  kbl( booktabs = T, caption = "Regression of Y2 HOLM Adjusted Comparison") %>% 
    kable_styling(latex_options = c("striped", "HOLD_position")) %>% 
    column_spec(c(3,4), width = "2cm")

```

* Hochberg Adjustments
    
```{r, echo = F}

hoch_data <- 
  sum2 %>% select(`Model Term`, `P-value`) %>% arrange(-`P-value`) %>% 
  filter(`Model Term` != "Intercept")

hoch_data$`Comparison P-value` <- 0.05
hoch_data$`Significant at Adj. Level` <- ""

cur_adj_n <- 1

for(i in 1:nrow(hoch_data)){
  
  cur_level <- 0.05 / cur_adj_n
  hoch_data[i,3] <- cur_level
 
  if(hoch_data[i,2] > cur_level){
    cur_adj_n <- cur_adj_n + 1
    
    holm_data[i,3] <- cur_level
  }
}

hoch_data[,4] <- ifelse(hoch_data[,2] < hoch_data[,3], "*", "")

hoch_data[,2:3] <- lapply(hoch_data[,2:3], round_3)

hoch_data %>% 
  kbl( booktabs = T, caption = "Regression of Y2 HOCHBERG Adjusted Comparison") %>% 
    kable_styling(latex_options = c("striped", "HOLD_position")) %>% 
    column_spec(c(3,4),  width = "2cm")

```

As a result of the review, there are no statistically significant predictors of Y2 when all variables 
are included. 

## 13.4 - C

In this section we will again do the two different models for Y1 and Y2, will look at the model 
selection plots, and summarize the final selected model. 

### Step Wise Regression on Y1 

Since we have 8 predictors, and for each predictor we have an option to include or not include
it into the model, we have a total of $2^8 = 256$ possible models we can create. Therefore, 
using built in functions, we obtain a subset of candidate models. We will evaluate these 
candidate models using adjusted R-squared and AIC metrics. There two metrics should provide us with 
the simplest and most effective models that explain the highest proportion of Y1 variance, after 
adjusting for the number of predictors in the model. 

Table below lists 8 candidate models with the best $R^2$ and $AIC$ scores. 

```{r}
k <- ols_step_best_subset(y1_lm1)

k %>% dplyr::select(n, predictors) %>% 
  kbl(booktabs = T, 
      caption = "Regression of Y1, Best Candidate Models") %>% 
  kable_styling(latex_options = c("striped", "HOLD_position"))
```

\newpage 

Values of metrics are presented on the plots below, each model is indexed according to the table
above. 

```{r, echo = F}
g1 <- 
  ggplot(data = k, 
         aes(x = n, y = adjr)) + geom_point(size = 1) + 
    geom_line(color = "blue") + 
    theme_minimal() + 
  xlab("Model Index") + 
  ylab("Adjusted R-square") +
  ggtitle("Regression of Y1, \n Adjusted R-square for \n Candidate Models")

g2 <- 
  ggplot(data = k, 
         aes(x = n, y = aic)) + geom_point(size = 1) + 
    geom_line(color = "blue")+ 
    theme_minimal() + 
  xlab("Model Index") + 
  ylab("AIC") +
  ggtitle("Regression of Y1, \n AIC for Candidate Models")

grid.arrange(g1, g2, nrow = 1)
```

It appears that model 3 is perhaps the best possible model that we can employ to explain variation 
in Y1. Metrics for this model are given below: 

```{r, echo = F}

k <- data.frame(k)

k_f <- 
  k %>% filter(n == 3) %>% 
  dplyr::select(predictors, rsquare, adjr, aic)

colnames(k_f) <- c("Predictors", "R-squared", "Adj. R-squared", "AIC")

k_f[,2:4] <- lapply(k_f[,2:4], round_3)

k_f %>% 
  kbl(caption = "Regression of Y1, Parameters of Selected Model", 
      booktabs = T) %>% 
  kable_styling(latex_options = c("HOLD_position"))

```

Therefore, we fit this model, and provide a summary table for its coefficients and 
other statistics: 

```{r, echo = F}
y1_lm1_step <- stepAIC(y1_lm1, direction = "both", 
                      trace = FALSE)

sum22 <- data.frame(summary(y1_lm1_step)$coefficients)

sum22$names <- c("Intercept", "Arm 6",  "Arm 8","Income >= $30K")

rownames(sum22) <- NULL

sum22 <- sum22 %>%  dplyr::select(names, everything())

round_3 <- function(x){round(x,3)}
sum22[,2:5] <- lapply(sum22[,2:5], round_3)

colnames(sum22) <-c("Model Term", "Estimate", "Std. Error", "T-value", "P-value") 

kbl(sum22, booktabs = T, caption = "Regression of Y1, ") %>% 
  kable_styling(latex_options = c("striped", "HOLD_position"))

```

```{r, echo = F}
a81 <- sum2[sum2$`Model Term` == "Arm 8", ]$Estimate
a82 <- sum22[sum22$`Model Term` == "Arm 8", ]$Estimate
```

After selecting the best possible predictors we observe different results: 

* All predictors selected are now statistically significantly related to the outcome variable. 

* The fact that p-values and coefficients changed implies that we had a multicollinearity 
  problem in the previously stated regression model with all possible predictors. 
  
* For example, coefficient for Arm 8 indicator variable changed from `r a81` to `r a82`
  + It can be of interest to us to identify which variables are correlated with Arm 8 indicator, 
    but it involves $\chi ^2$ testing for correlation of categorical variables 
    
  + We will present more on multicollinearity in the appendix

### Step Wise Regression on Y2

Similarly, there are 256 possible models we can create to explain variance of Y2, and here are 
some of the best candidate models 

```{r}
k <- ols_step_best_subset(y2_lm1)

k %>% dplyr::select(n, predictors) %>% 
  kbl(booktabs = T, 
      caption = "Regression of Y2, Best Candidate Models") %>% 
  kable_styling(latex_options = c("striped", "HOLD_position"))

```

Using built in functions, we can get Adjusted R-square and AIC for each candidate model

```{r, echo = F}
g1 <- 
  ggplot(data = k, 
         aes(x = n, y = adjr)) + geom_point(size = 1) + 
    geom_line(color = "blue") + 
    theme_minimal() + 
  xlab("Model Index") + 
  ylab("Adjusted R-square") +
  ggtitle("Regression of Y2, \n Adjusted R-square for \n Candidate Models")

g2 <- 
  ggplot(data = k, 
         aes(x = n, y = aic)) + geom_point(size = 1) + 
    geom_line(color = "blue")+ 
    theme_minimal() + 
  xlab("Model Index") + 
  ylab("AIC") +
  ggtitle("Regression of Y2, \n AIC for Candidate Models")

grid.arrange(g1, g2, nrow = 1)
```

It appears that a model with index 2 is the best possible candidate

```{r, echo = F}

k <- data.frame(k)

k_f <- 
  k %>% filter(n == 2) %>% 
  dplyr::select(predictors, rsquare, adjr, aic)

colnames(k_f) <- c("Predictors", "R-squared", "Adj. R-squared", "AIC")

k_f[,2:4] <- lapply(k_f[,2:4], round_3)

k_f %>% 
  kbl(caption = "Regression of Y2, Parameters of Selected Model", 
      booktabs = T) %>% 
  kable_styling(latex_options = "HOLD_position")

```

```{r, echo = F}
y2_lm1_step <- stepAIC(y2_lm1, direction = "both", 
                      trace = FALSE)

sum2 <- data.frame(summary(y2_lm1_step)$coefficients)

sum2$names <- c("Intercept", "Arm 7", "Income >= $30K")

rownames(sum2) <- NULL

sum2 <- sum2 %>%  dplyr::select(names, everything())

round_3 <- function(x){round(x,3)}
sum2[,2:5] <- lapply(sum2[,2:5], round_3)

colnames(sum2) <-c("Model Term", "Estimate", "Std. Error", "T-value", "P-value") 

kbl(sum2, booktabs = T, caption = "") %>% 
  kable_styling(latex_options = c("striped", "HOLD_position"))

```
After fitting a reduced model we, once again, obtain different results. Interesting to see that 
even though higher income indicator is not statistically significant, it is still included into the
final model. 

The fact that coefficient almost doubled for Arm 7 indicator variable may suggest that we have a high
degree of collinearity in the data set. Perhaps, that is the reason why the best possible 
model only includes 2 out of 8 predictors. 

\newpage 

# Appendix: 12.2 

To be finished after the midterm 

```{r, include = F}
# look at the correlation between age and age^2 
ggplot(data = infants_f, 
       aes(x = m_age, 
           y = m_age^2 )) + geom_point() + 
  
  stat_smooth(method = "lm", se = F, aes(color = "Fitted Regression Line")) + 
  scale_color_manual(values = c("Fitted Regression Line" = "red")) + 
  
  xlab("Age") + 
  ylab("Age Squared") + 
  ggtitle(paste("Correlation: ", round(cor(infants_f$m_age, infants_f$m_age^2),4))) + 
  theme_minimal()


# now apply centering: 

infants_f$m_age_centered <- with(infants_f, m_age - mean(m_age))
infants_f$gest_weeks_centered <- with(infants_f, gest_weeks - mean(gest_weeks))

ggplot(data = infants_f, 
       aes(x = m_age_centered, 
           y = m_age_centered^2 )) + geom_point() +  
  
  stat_smooth(method = "lm", se = F, aes(color = "Fitted Regression Line")) + 
  scale_color_manual(values = c("Fitted Regression Line" = "red")) + 
  
  xlab("Age") + 
  ylab("Age Squared") + 
  ggtitle(paste("Correlation: ", round(cor(infants_f$m_age_centered, infants_f$m_age_centered^2),4))) + 
  theme_minimal()

```


```{r, include = F}
ggplot(data = infants_f, 
       aes(x = m_age, 
           y = birth_w)) + geom_point() + 
  
  stat_smooth(se = F, aes(color = "Smooth Trend Line")) + 
  stat_smooth(se = F, method = "lm", aes(color = "Regression Line")) + 
  
  scale_color_manual(values = c("Smooth Trend Line" = "blue", 
                                "Regression Line" = "red")) + 
  xlab("Mother's Age") + 
  ylab("Infant's Birth Weight") + 
  ggtitle(paste("Correlation Between Mother's Age  \n and Infant's Birth Weight: ", 
                round(cor(infants_f$m_age, 
                          infants_f$birth_w), 4))) + 
  theme_minimal()
  
ggplot(data = infants_f, 
       aes(x = m_age_centered, 
           y = birth_w)) + geom_point() + 
  
  stat_smooth(se = F, aes(color = "Smooth Trend Line")) + 
  stat_smooth(se = F, method = "lm", aes(color = "Regression Line")) + 
  
  scale_color_manual(values = c("Smooth Trend Line" = "blue", 
                                "Regression Line" = "red")) + 
  xlab("Mother's Age") + 
  ylab("Infant's Birth Weight") + 
  ggtitle(paste("Correlation Between Centered Mother's Age  \n and Infant's Birth Weight: ", 
                round(cor(infants_f$m_age_centered, 
                          infants_f$birth_w), 4))) + 
  theme_minimal()

```













---
title: "Exam 2"
author: "Denis Ostroushko"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    extra_dependencies: ["float"]
editor_options: 
  markdown: 
    wrap: 72
---



```{r, echo = F}
knitr::opts_chunk$set(echo = F, message = F, warning = F, fig.pos = "!H", fig.height=4, fig.width=7, fig.align='center')
options(scipen=999)
```

```{r load all packages from the master file , include=F}
source('/Users/denisostroushko/Desktop/UofM MS/MS Fall 2022/Puhb 7405/Package master list .R')
```

# Problem 1

```{r}

p1_data <- read_xlsx("/Users/denisostroushko/Desktop/UofM MS/MS Fall 2022/Puhb 7405/Data Sets/Exam 2/E-CIG-2-E-22.xlsx")

p1_data$Z <- with(p1_data, ifelse(arm == 6, 1, 0))

p1_data$X1 <- log(p1_data$NNAL_vt0_creat)
p1_data$X2 <- log(p1_data$TNE_vt0_creat)

p1_data_f <- p1_data %>% select(subj_id, Z, X1, X2, arm) %>% na.omit()

# 
```

We begin this problem by summarizing the data available to us, and continue this summary into the **1 - A** Section. 
The data set contains `r nrow(p1_data)` observations for `r length(unique(p1_data$subj_id))` participants in the study.

One of the participants has a missing value of baseline NNAL measurement. This variable is important for our 
analysis, therefore we will omit this observation. The final data set includes `r nrow(p1_data_f)` observations. 

### 1 - A

Problem 1-A asks us to fit a logistic regression model using two log-transformed baseline measurements. We are interested in evaluating how the two variables are balanced between the two experiment arms, arm 5 and arm 6. 

```{r}
p1_data_f %>% 
  group_by(arm) %>% 
  summarize(N = n(), 
            
            mean_nnal = mean(X1),
            sd_nnal = sd(X1),
            
            mean_tne = mean(X2),
            sd_tne = sd(X2)
            ) %>% 
  kable(
    booktabs = T, 
    col.names = c("Arm", "N", "Mean", "SD", "Mean", "SD"), 
    align = c("l", rep('c', (length(p1_data_f)-1)))
  ) %>% 
  kable_styling(
    latex_options = c("HOLD_position", "striped")
  ) %>% 
  
  add_header_above(c(" " = 2, "Baseline Log NNAL" = 2, "Baseline Log TNE" = 2))

```

Table above provides mean and standard deviation for the two variables we want to use in the propensity score model. 
We can see that the means of the baseline NNAL measurements appear to be visually different between the two groups,
although, it with a high standard deviation difference may be due to the noise abd variation of the data. 

Average TNE measurements appear to be quite similar for both treatment arms. 

In order to investigate the difference in distribtuion of two variables further we refer to the histograms. 

```{r}

ggplot(data = p1_data_f, 
       aes(x = X1)) + 
  geom_histogram(data = p1_data_f %>% filter(Z == 1), aes(fill = "Arm 6"), color = "dark blue", alpha = .75) +
  geom_histogram(data = p1_data_f %>% filter(Z == 0), aes(fill = "Arm 5"), color = "dark blue", alpha = .75) +

  theme_minimal() + 
  
  ggtitle("Baseline Log NNAL Measurements \n Between Two Groups") + 
  ylab("Count") + 
  xlab("Log NNAL") 

```
It appears that the shape of two distributions is quite similar for the two treatment arms. There are quite more 
observations with the log-NNAL measurements above 1.5 for treatment arm 6, which may be the reason as to why the mean
of observations is higher for this group. We also have about twice the amount of observations in the treatment 
group 6, so, perhaps, if we are able to observe 30 more people who can qualify to be in treatment arm 5, we 
can observe more extreme values, and more values that would tend toward the center of the distribution, making
the two distributions very similar. 


```{r}

ggplot(data = p1_data_f, 
       aes(x = X2)) + 
  geom_histogram(data = p1_data_f %>% filter(Z == 1), aes(fill = "Arm 6"), color = "dark blue", alpha = .75) +
  geom_histogram(data = p1_data_f %>% filter(Z == 0), aes(fill = "Arm 5"), color = "dark blue", alpha = .75) +

  theme_minimal() + 
  
  ggtitle("Baseline Log TNE Measurements \n Between Two Groups") + 
  ylab("Count") + 
  xlab("Log TNE")


```
The distributions of log-TNE values between the two groups appear quite different, but also have similar features. 
We can see that the distribution have heavy tails on the left, with the 'center' of each distribution being on 
the right side. Perhaps, this is caused by the logarithmic transformation. Overall, it is not easy to gauge 
the similarity of two distributions here due to varying sample size and natural variations of these biomarkers. 

We are now ready to fit the logistic regression model to obtain propensity scores for each subject. 
Our response variable is Z, a binary, where $Z = 1$ if a study participant is in the treatment arm 6, and 0 otherwise.

Therefore, the model statement is: 

$$\Large ln \frac{P(Z = 1)}{1 - P(Z = 1)} = \hat \beta_0 + \hat \beta_1 * X_1 + \hat \beta_2 * X_2$$
Where $X_1$ is a baseline measurement of NNAL on the natural logarithmic scale, and $X_2$ is a baseline measurement 
of TNE on the natural logarithmic scale

```{r}

p1_glm <- glm(Z ~ X1 + X2, data = p1_data_f, family = "binomial")

p1_data_f$prop_score <- p1_glm$fitted.values

```

After fitting the model, we summarize obtained propensity scores for each experiment arm. 

```{r}
p1_data_f %>% 
  group_by(arm) %>% 
  summarize(N = n(), 
            
            mean_nnal = mean(prop_score),
            sd_nnal = sd(prop_score)
            ) %>% 
  kable(
    booktabs = T, 
    col.names = c("Arm", "N", "Mean", "SD"), 
    align = c("l", rep('c', (length(p1_data_f)-1)))
  ) %>% 
  kable_styling(
    latex_options = c("HOLD_position", "striped")
  ) %>% 
  
  add_header_above(c(" " = 2, "Propensity Score" = 2))

```

It appears that the two samples have mean and standard deviations that are quite similar. We can aso investigate the 
shape of distributions for each treatment group. 

```{r}

ggplot(data = p1_data_f, 
       aes(x = prop_score)) + 
  geom_histogram(data = p1_data_f %>% filter(Z == 1), aes(fill = "Arm 6"), color = "dark blue", alpha = .75) +
  geom_histogram(data = p1_data_f %>% filter(Z == 0), aes(fill = "Arm 5"), color = "dark blue", alpha = .75) +

  theme_minimal() + 
  
  ggtitle("Baseline Log TNE Measurements \n Between Two Groups") + 
  ylab("Count") + 
  xlab("Log TNE")

```

### 1 - B

Using propensity scores we calculate the odds of receiving the treatment for each experiment subject on the logarithmic 
scale. We give the summary of these odds for each sample in the table below: 

```{r}
p1_data_f$Y <- log(with(p1_data_f, prop_score/(1-prop_score)))
```



```{r}
p1_data_f %>% 
  group_by(arm) %>% 
  summarize(N = n(), 
            
            mean_nnal = mean(Y),
            sd_nnal = sd(Y)
            ) %>% 
  kable(
    booktabs = T, 
    col.names = c("Arm", "N", "Mean", "SD"), 
    align = c("l", rep('c', (length(p1_data_f)-1)))
  ) %>% 
  kable_styling(
    latex_options = c("HOLD_position", "striped")
  ) %>% 
  
  add_header_above(c(" " = 2, "Log-Odds" = 2))
  
```

Because log-odds are a function of propensity scores, same conclusions apply here again. 

We can compare the average $Y$ values using a $t$-test. Before doing so, we can do two checks: 

1. Use box plots to make sure there are no influential outliers. In case there are many influential outliers, 
  we can pivot to a non-parametric Wilcoxon test. However, I suspect that we will not observe many influential 
  outliers on the histograms above
  
2. Compare the variances of the two samples using an $F-$test. We will statistically test if the ratio of variances 
  is greatly different from 1. In case the ratios are statistically different we can pivot to the Wilcoxon test as well.

Boxplot below shows no visual evidence of greatly influential outliers. 

```{r}

ggplot(data = p1_data_f, 
       aes(
        x = as.factor(arm), 
        y = Y, 
        group = arm)) + 
  geom_boxplot() + 
  geom_jitter() + 
  theme_minimal() +
  
  ylab("Treatment Odds")+
  xlab("Experiment Arm") 

```

We can now carry out an F test to check the difference between variances of treatment odds between the two groups. 
We perform the test on the $\alpha$ = 0.05 significance level. 

```{r}

df_1 <- nrow(p1_data_f %>% filter(Z == 1)) - 1
df_2 <- nrow(p1_data_f %>% filter(Z == 0)) - 1

s_1 <- sd(p1_data_f[p1_data_f$Z == 1,]$Y)^2
s_2 <- sd(p1_data_f[p1_data_f$Z == 0,]$Y)^2

F_stat <- s_1 / s_2 # also a ratio that we infer from 

ci_low <- F_stat * qf(p = 0.05/2, df1 = df_1, df2 = df_2)
ci_upp <- F_stat * 1/ qf(p = 0.05/2, df1 = df_1, df2 = df_2) # for the upper bound we simply swap df2 and df1

Cutoff_F <- qf(p = 1 - 0.05/2, df1 = df_1, df2 = df_2)

P_val <- 1 - pf(F_stat, df1 = df_1, df2 = df_2)

```

Formal statement for $F-$test on the $\alpha=$ 0.05 level is below: 

*   Variance of treatment arm 5 is $\large s^2_1 =$ `r round(s_2, 4)` and variance of treatment arm 6 is 
    $\large s^2_2 =$ `r round(s_1, 4)`. The ratio $\large s^2_2 / s^2_1$ is `r round(F_stat, 4)`
    
*   Null Hypothesis: $\large H_0: s^2_1 = s^2_2$

*   Alternative Hypothesis: $\large H_a: s^2_1 \neq s^2_2$

*   Test statistic $F =$ `r round(F_stat, 4)` 

*   Critical cutoff $F-$value on `r df_1` and `r df_2` degrees of freedom is `r round(Cutoff_F, 4)`

*   $P(F^* > F) =$ `r round(P_val, 4)`

*   Conclusion: $F-$ statistic is smaller than the critical value, p-value is much bigger than the accepted 
    cutoff 0.05, so there is not enough evidence to reject the null hypothesis and conclude that the the variances 
    of two samples is not equal. 
    
So, we confirmed that the variances do not differ greatly using a statistical test and visually confirmed that there 
are no influential outliers in the two samples. Therefore, we can conduct a $t-$ test to comapre the average
treatment odds. 

```{r}

t_res <- t.test(x = p1_data_f[p1_data_f$Z == 1, ]$Y, 
                y = p1_data_f[p1_data_f$Z == 0, ]$Y)

mean1 <- mean(p1_data_f[p1_data_f$Z == 1, ]$Y)
mean2 <- mean(p1_data_f[p1_data_f$Z == 0, ]$Y)
  
difference = mean1 - mean2

```

Formal statement for $t-$test at the $\alpha$ = 0.05 significance level is below: 

*   Average log-odds for treatment arm 5 is $\bar X_1 =$ `r round(mean2, 4)`, average log-odds for treatment arm 6 is 
    $\bar X_2 =$ `r round(mean1, 4)`, observed difference $\bar X_2 - \bar X_1$ is `r round(difference, 4)`
    
*   Null Hypothesis $H_0: \bar X_1 = \bar X_2$

*   Alternative Hypothesis $H_a: \bar X_1 \neq \bar X_2$

*   Test $T-$ statistics is `r round(t_res$statistic, 4)` 

*   $P(T^* > T) =$ `r round(t_res$p.value,4)`

*   Conclusion: $T-$ statistics is smaller than the critical cutoff, so we can not reject the Null Hypothesis
    and conclude that the treatment log-odds are different between the two groups. However, p-value is only about 2.5
    times greater than the accepted significance level, so these results can be suggestive that $\bar X_2$ is 
    greater. We should not disregard these results easily. 
    
    This is also supported by the confidence interval for the difference estimate, which contains 
    mostly positive values. C.I. is given by (`r round(t_res$conf.int[1], 4)`, `r round(t_res$conf.int[2], 4)`). 
    
    


### 1 - C


Based on some literature review, many psychology and social science data analysis methods refer to this method as 
Cohen's d. All sources I reviewed state that in practice effect sizes between 0.2 and 0.5 are considered as Medium 
size. 

A good explanation is given here: 
https://datatab.net/tutorial/effect-size-independent-t-test

We calculate the effect size as the difference divided by the pooled variance, where pooled variance is given by: 

$$\Large s^2_p = \frac{(n_1 - 1) * s^2_1 + (n_2 - 1) * s^2_2}{n_2 + n_1 - 2}$$
And effect size is given by: 

$$\Large E.S. = \frac{\bar X_2 - \bar X_1}{\sqrt{S^2_p}}$$

```{R}

pooled_variance <- 
  (df_1 * s_1 + df_2 * s_2) / (df_1 + df_2)

effect_size <- 
  difference / sqrt(pooled_variance)

```

I will rely on a test instead because we have more accessible and straightforward way to get a confidence interval, that is 
widely known and accepted among the research community. 

However, effect size is also a good method that gives us an intuitive tool for inference. We took the ratio of a difference 
to the common, or pooled, standard deviation. With our estimate of `r round(effect_size,4)`, we have a moderate effect size. 
Also, difference between samples is approximately `r 100*round(effect_size,2)`% of the common standard deviation. So, the effect is 
, perhaps, "hidden" due to great variance in the sample, or it is "diluted" due to the variance that occurs in the sample. 

Another interpretation is that the difference between the two groups is small when compared to the average, or pooled, variance 
in the sample. There is too much variance to make conclusive statements. 

\newpage 

# Problem 2 

```{r}
p2_data <- read_xlsx("/Users/denisostroushko/Desktop/UofM MS/MS Fall 2022/Puhb 7405/Data Sets/Exam 2/BMT-Trial-Orig.xlsx")

p2_data_f <- na.omit(p2_data)
  
```

The data set for this problem contains `r nrow(p2_data)` observations. In the context of the survival analysis problem, we look 
at the time to event, which is defined by the `duration` variable. There are `r length(which(is.na(p2_data$duration)))` 
observations in the data set with the missing time to event measure, so they will be removed from the analysis. 
Therefore, all survival models will be fit using a sample of `r nrow(p2_data_f)` study participants. 

### 2 - A

Before obtaining the model estimate it is helpful to develop intuition using graphical methods. We can see that the two groups 
have very similar fitted survival curves. Therefore, we should not expect to see a big estimate of hazard ratio. However, 
we can see that the median survival time is quite larger for the Peripheral Blood Stem Cells (treatment 2) treatment group. 


```{r}
fit <- survfit(Surv(duration, status) ~ Treatment, data = p2_data)

plot <- 
  ggsurvplot(fit, data = p2_data, surv.median.line = "hv") 
  # ggtitle(paste("Kaplan Meier Survival Curves. \n   Cases Median Survival Time: ", case_median_t, 
  #               "days. \n   Controls Median Survival Time: ", cont_median_t, " days.")) 
plot$plot
```

Now we can fit a Cox Proportional Hazard Regression Model to compare survival likelihood between the two groups. 
Model estiamte is given below: 

```{r}

surv_m1 <- coxph(Surv(duration, status) ~ Treatment, data = p2_data)

res_reg <- data.frame(summary(surv_m1)$coefficients)

res_reg$var <- rownames(res_reg)
rownames(res_reg) <- NULL
res_reg <- res_reg %>% select(var, everything())

res_reg <-
  res_reg %>% mutate_at(vars(coef, exp.coef., se.coef., z, `Pr...z..`),
                                 funs(round(., 6)
                                      )
                                 )

colnames(res_reg) <- c("Predictor", "Estiamte", "Exponentiated Estiamte", "Standard Error", "Z Value", "P value")
res_reg %>%
  kbl(booktabs = T, align = c('l','c', 'c', 'c', 'c', 'c')) %>%
  kable_styling(latex_options = c("striped", "HOLD_position"))

c <- res_reg$`Exponentiated Estiamte`
```

*   We defined group who received Bone Marrow (BM) treatment as a reference level, 
    and a group who received Peripheral Blood Stem Cells (PBSC) treatment as a comparison group

*   As we can see, exponentiated coefficient, a hazard ratio, is `r round(c, 4)`, which means that the PBSC treatment group 
    has approximately `r (round(c, 4) - 1) * 100`% higher chance of having a leukemia related death at any point in time. 
    
*   High p-value indicates that this difference is not statistically significant, so we cannot reject null hypothesis. 
    Therefore, PBSC treatment does not improve survival chances when compared to Bone Marrow treatment for the sample of 
    leukemia patients. 
    
*   In addition, standard error is quite large in comparison to the coefficient, the coefficient itself is close to zero, 
    therefore there is a high degree of uncertainty in the estimate. 

### 2 - B 

We can how fit a Cox Proportional Hazard Regression Model with all predictors. 
A summary table with model estimates is given below: 

```{r}

surv_m2 <- coxph(Surv(duration, status) ~ 
                    GENDER + age + Treatment + Race  + `HLA-Match`, data = p2_data)

res_reg2 <- data.frame(summary(surv_m2)$coefficients)

res_reg2$var <- rownames(res_reg2)
rownames(res_reg2) <- NULL
res_reg2 <- res_reg2 %>% select(var, everything())

res_reg2 <-
  res_reg2 %>% mutate_at(vars(coef, exp.coef., se.coef., z, `Pr...z..`),
                                 funs(round(., 6)
                                      )
                                 )

colnames(res_reg2) <- c("Predictor", "Estiamte", "Exponentiated Estiamte", "Standard Error", "Z Value", "P value")

res_reg2 %>%
  kbl(booktabs = T, align = c('l','c', 'c', 'c', 'c', 'c')) %>%
  kable_styling(latex_options = c("striped", "HOLD_position"))

a <- res_reg2[res_reg2$Predictor == "age", ]$`Exponentiated Estiamte`
h <- res_reg2[res_reg2$Predictor == "`HLA-Match`", ]$`Exponentiated Estiamte`

```

```{r}
ci <- data.frame(exp(confint(surv_m2)))
ci$var <- rownames(ci)

a_l <- ci[ci$var == "age", ]$X2.5..

a_u <- ci[ci$var == "age", ]$X97.5..

```
*   Age and HLA-Match are two statistically significant predictors of the survival chance for leukemia patients

*   Exponentiated coefficient for Age is `r round(a, 4)` which means that one additional year of age multiplicatively increases 
    the chance of dying at any point in time by `r round(a, 4)`, or by approximately `r 100*(round(a, 4) - 1)`% 

    However, it makes more sense to assess age effects on a large time frame. For example, additional 10 years of age 
    increase the chance of having a leukemia related death at point in time by `r 10 * 100*(round(a, 4) - 1)`%, after 
    adjusting for the effects of other predictors 
    
    This estimate is bounded by the (`r 10*100*(round(a_l, 4) - 1)`%, `r 10*100*(round(a_u, 4) - 1)`%) 95% confidence interval. 
    Due to exponentiation of the coefficient, confidence interval appears to be skewed to the right. 
    
*   HLA-Match has two possible values. Patients with score 5 are chosen as a reference level for this modeling exercise, 
    while patients with score 6 were used as the comparison group. 
    
    Patients with HAL score 6 were have a hazard ratio of `r round(h, 4)`, which means that patients in this group had a much 
    lower chance of dying. In fact, at any point in time, patients in group with score 6 had approximately 
    `r 100*(round(h,4)-1)`% lower chance of dying, after adjusting for other predictors. 

*   Looking at raw coefficients for these predictors, we can also have more confidence in these predictors because standard errors
    are small in comparison with the coefficient magnitude and size. Of course, confidence intervals are constructed using these 
    standard errors, but it is also nice to see this information in the table next to the model output. 

### 2 - C

Table below provides a side by side comparison of the Treatment coefficient from the two regression models. 
Recall that the coefficient is given for the comparison of PBSC group to the BM group. 

```{r}

part_c <- 
  
  cbind(
  
    data.frame(
      Model = c("Full", "Treatment Only")
    ), 
    rbind(
      res_reg2 %>% filter(Predictor == "Treatment"),
      res_reg %>% filter(Predictor == "Treatment")
    )
  )

part_c %>%
  kbl(booktabs = T, align = c('l','c', 'c', 'c', 'c', 'c')) %>%
  kable_styling(latex_options = c("striped", "HOLD_position"))


```

*   The two estimates are not the same, they do not have the same p-value and standard errors. However, they are also not 
    drastically different and do not tell a different story. 

*   It is interesting to observe that the coefficient actually increased in magnitede after the addition of extra 
    predictors into the model. 
    
*   



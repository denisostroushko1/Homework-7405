---
title: "Exam 2"
author: "Denis Ostroushko"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    extra_dependencies: ["float"]
editor_options: 
  markdown: 
    wrap: 72
---



```{r, echo = F}
knitr::opts_chunk$set(echo = F, message = F, warning = F, fig.pos = "!H", fig.height=4, fig.width=7, fig.align='center')
options(scipen=999)
```

```{r load all packages from the master file , include=F}
source('/Users/denisostroushko/Desktop/UofM MS/MS Fall 2022/Puhb 7405/Package master list .R')
```

# Problem 1

```{r}

p1_data <- read_xlsx("/Users/denisostroushko/Desktop/UofM MS/MS Fall 2022/Puhb 7405/Data Sets/Exam 2/E-CIG-2-E-22.xlsx")

p1_data$Z <- with(p1_data, ifelse(arm == 6, 1, 0))

p1_data$X1 <- log(p1_data$NNAL_vt0_creat)
p1_data$X2 <- log(p1_data$TNE_vt0_creat)

p1_data_f <- p1_data %>% select(subj_id, Z, X1, X2, arm) %>% na.omit()

# 
```

We begin this problem by summarizing the data available to us, and continue this summary into the **1 - A** Section. 
The data set contains `r nrow(p1_data)` observations for `r length(unique(p1_data$subj_id))` participants in the study.

One of the participants has a missing value of baseline NNAL measurement. This variable is important for our 
analysis, therefore we will omit this observation. The final data set includes `r nrow(p1_data_f)` observations. 

### 1 - A

Problem 1-A asks us to fit a logistic regression model using two log-transformed baseline measurements. We are interested in evaluating how the two variables are balanced between the two experiment arms, arm 5 and arm 6. 

```{r}
p1_data_f %>% 
  group_by(arm) %>% 
  summarize(N = n(), 
            
            mean_nnal = mean(X1),
            sd_nnal = sd(X1),
            
            mean_tne = mean(X2),
            sd_tne = sd(X2)
            ) %>% 
  kable(
    booktabs = T, 
    col.names = c("Arm", "N", "Mean", "SD", "Mean", "SD"), 
    align = c("l", rep('c', (length(p1_data_f)-1)))
  ) %>% 
  kable_styling(
    latex_options = c("HOLD_position", "striped")
  ) %>% 
  
  add_header_above(c(" " = 2, "Baseline Log NNAL" = 2, "Baseline Log TNE" = 2))

```

Table above provides mean and standard deviation for the two variables we want to use in the propensity score model. 
We can see that the means of the baseline NNAL measurements appear to be visually different between the two groups,
although, it with a high standard deviation difference may be due to the noise abd variation of the data. 

Average TNE measurements appear to be quite similar for both treatment arms. 

In order to investigate the difference in distribtuion of two variables further we refer to the histograms. 

```{r}

ggplot(data = p1_data_f, 
       aes(x = X1)) + 
  geom_histogram(data = p1_data_f %>% filter(Z == 1), aes(fill = "Arm 6"), color = "dark blue", alpha = .75) +
  geom_histogram(data = p1_data_f %>% filter(Z == 0), aes(fill = "Arm 5"), color = "dark blue", alpha = .75) +

  theme_minimal() + 
  
  ggtitle("Baseline Log NNAL Measurements \n Between Two Groups") + 
  ylab("Count") + 
  xlab("Log NNAL") 

```
It appears that the shape of two distributions is quite similar for the two treatment arms. There are quite more 
observations with the log-NNAL measurements above 1.5 for treatment arm 6, which may be the reason as to why the mean
of observations is higher for this group. We also have about twice the amount of observations in the treatment 
group 6, so, perhaps, if we are able to observe 30 more people who can qualify to be in treatment arm 5, we 
can observe more extreme values, and more values that would tend toward the center of the distribution, making
the two distributions very similar. 


```{r}

ggplot(data = p1_data_f, 
       aes(x = X2)) + 
  geom_histogram(data = p1_data_f %>% filter(Z == 1), aes(fill = "Arm 6"), color = "dark blue", alpha = .75) +
  geom_histogram(data = p1_data_f %>% filter(Z == 0), aes(fill = "Arm 5"), color = "dark blue", alpha = .75) +

  theme_minimal() + 
  
  ggtitle("Baseline Log TNE Measurements \n Between Two Groups") + 
  ylab("Count") + 
  xlab("Log TNE")


```
The distributions of log-TNE values between the two groups appear quite different, but also have similar features. 
We can see that the distribution have heavy tails on the left, with the 'center' of each distribution being on 
the right side. Perhaps, this is caused by the logarithmic transformation. Overall, it is not easy to gauge 
the similarity of two distributions here due to varying sample size and natural variations of these biomarkers. 

We are now ready to fit the logistic regression model to obtain propensity scores for each subject. 
Our response variable is Z, a binary, where $Z = 1$ if a study participant is in the treatment arm 6, and 0 otherwise.

Therefore, the model statement is: 

$$\Large ln \frac{P(Z = 1)}{1 - P(Z = 1)} = \hat \beta_0 + \hat \beta_1 * X_1 + \hat \beta_2 * X_2$$
Where $X_1$ is a baseline measurement of NNAL on the natural logarithmic scale, and $X_2$ is a baseline measurement 
of TNE on the natural logarithmic scale

```{r}

p1_glm <- glm(Z ~ X1 + X2, data = p1_data_f, family = "binomial")

p1_data_f$prop_score <- p1_glm$fitted.values

```

After fitting the model, we summarize obtained propensity scores for each experiment arm. 

```{r}
p1_data_f %>% 
  group_by(arm) %>% 
  summarize(N = n(), 
            
            mean_nnal = mean(prop_score),
            sd_nnal = sd(prop_score)
            ) %>% 
  kable(
    booktabs = T, 
    col.names = c("Arm", "N", "Mean", "SD"), 
    align = c("l", rep('c', (length(p1_data_f)-1)))
  ) %>% 
  kable_styling(
    latex_options = c("HOLD_position", "striped")
  ) %>% 
  
  add_header_above(c(" " = 2, "Propensity Score" = 2))

```

It appears that the two samples have mean and standard deviations that are quite similar. We can aso investigate the 
shape of distributions for each treatment group. 

```{r}

ggplot(data = p1_data_f, 
       aes(x = prop_score)) + 
  geom_histogram(data = p1_data_f %>% filter(Z == 1), aes(fill = "Arm 6"), color = "dark blue", alpha = .75) +
  geom_histogram(data = p1_data_f %>% filter(Z == 0), aes(fill = "Arm 5"), color = "dark blue", alpha = .75) +

  theme_minimal() + 
  
  ggtitle("Baseline Log TNE Measurements \n Between Two Groups") + 
  ylab("Count") + 
  xlab("Log TNE")

```

### 1 - B

Using propensity scores we calculate the odds of receiving the treatment for each experiment subject on the logarithmic 
scale. We give the summary of these odds for each sample in the table below: 

```{r}
p1_data_f$Y <- log(with(p1_data_f, prop_score/(1-prop_score)))
```



```{r}
p1_data_f %>% 
  group_by(arm) %>% 
  summarize(N = n(), 
            
            mean_nnal = mean(Y),
            sd_nnal = sd(Y)
            ) %>% 
  kable(
    booktabs = T, 
    col.names = c("Arm", "N", "Mean", "SD"), 
    align = c("l", rep('c', (length(p1_data_f)-1)))
  ) %>% 
  kable_styling(
    latex_options = c("HOLD_position", "striped")
  ) %>% 
  
  add_header_above(c(" " = 2, "Log-Odds" = 2))
  
```

Because log-odds are a function of propensity scores, same conclusions apply here again. 

We can compare the average $Y$ values using a $t$-test. Before doing so, we can do two checks: 

1. Use box plots to make sure there are no influential outliers. In case there are many influential outliers, 
  we can pivot to a non-parametric Wilcoxon test. However, I suspect that we will not observe many influential 
  outliers on the histograms above
  
2. Compare the variances of the two samples using an $F-$test. We will statistically test if the ratio of variances 
  is greatly different from 1. In case the ratios are statistically different we can pivot to the Wilcoxon test as well.

Boxplot below shows no visual evidence of greatly influential outliers. 

```{r}

ggplot(data = p1_data_f, 
       aes(
        x = as.factor(arm), 
        y = Y, 
        group = arm)) + 
  geom_boxplot() + 
  geom_jitter() + 
  theme_minimal() +
  
  ylab("Treatment Odds")+
  xlab("Experiment Arm") 

```

We can now carry out an F test to check the difference between variances of treatment odds between the two groups. 
We perform the test on the $\alpha$ = 0.05 significance level. 

```{r}

df_1 <- nrow(p1_data_f %>% filter(Z == 1)) - 1
df_2 <- nrow(p1_data_f %>% filter(Z == 0)) - 1

s_1 <- sd(p1_data_f[p1_data_f$Z == 1,]$Y)^2
s_2 <- sd(p1_data_f[p1_data_f$Z == 0,]$Y)^2

F_stat <- s_1 / s_2 # also a ratio that we infer from 

ci_low <- F_stat * qf(p = 0.05/2, df1 = df_1, df2 = df_2)
ci_upp <- F_stat * 1/ qf(p = 0.05/2, df1 = df_1, df2 = df_2) # for the upper bound we simply swap df2 and df1

Cutoff_F <- qf(p = 1 - 0.05/2, df1 = df_1, df2 = df_2)

P_val <- 1 - pf(F_stat, df1 = df_1, df2 = df_2)

```

Formal statement for $F-$test on the $\alpha=$ 0.05 level is below: 

*   Variance of treatment arm 5 is $\large s^2_1 =$ `r round(s_2, 4)` and variance of treatment arm 6 is 
    $\large s^2_2 =$ `r round(s_1, 4)`. The ratio $\large s^2_2 / s^2_1$ is `r round(F_stat, 4)`
    
*   Null Hypothesis: $\large H_0: s^2_1 = s^2_2$

*   Alternative Hypothesis: $\large H_a: s^2_1 \neq s^2_2$

*   Test statistic $F =$ `r round(F_stat, 4)` 

*   Critical cutoff $F-$value on `r df_1` and `r df_2` degrees of freedom is `r round(Cutoff_F, 4)`

*   $P(F^* > F) =$ `r round(P_val, 4)`

*   Conclusion: $F-$ statistic is smaller than the critical value, p-value is much bigger than the accepted 
    cutoff 0.05, so there is not enough evidence to reject the null hypothesis and conclude that the the variances 
    of two samples is not equal. 
    
So, we confirmed that the variances do not differ greatly using a statistical test and visually confirmed that there 
are no influential outliers in the two samples. Therefore, we can conduct a $t-$ test to comapre the average
treatment odds. 

```{r}

t_res <- t.test(x = p1_data_f[p1_data_f$Z == 1, ]$Y, 
                y = p1_data_f[p1_data_f$Z == 0, ]$Y)

mean1 <- mean(p1_data_f[p1_data_f$Z == 1, ]$Y)
mean2 <- mean(p1_data_f[p1_data_f$Z == 0, ]$Y)
  
difference = mean1 - mean2

```

Formal statement for $t-$test at the $\alpha$ = 0.05 significance level is below: 

*   Average log-odds for treatment arm 5 is $\bar X_1 =$ `r round(mean2, 4)`, average log-odds for treatment arm 6 is 
    $\bar X_2 =$ `r round(mean1, 4)`, observed difference $\bar X_2 - \bar X_1$ is `r round(difference, 4)`
    
*   Null Hypothesis $H_0: \bar X_1 = \bar X_2$

*   Alternative Hypothesis $H_a: \bar X_1 \neq \bar X_2$

*   Test $T-$ statistics is `r round(t_res$statistic, 4)` 

*   $P(T^* > T) =$ `r round(t_res$p.value,4)`

*   Conclusion: $T-$ statistics is smaller than the critical cutoff, so we can not reject the Null Hypothesis
    and conclude that the treatment log-odds are different between the two groups. However, p-value is only about 2.5
    times greater than the accepted significance level, so these results can be suggestive that $\bar X_2$ is 
    greater. We should not disregard these results easily. 
    
    This is also supported by the confidence interval for the difference estimate, which contains 
    mostly positive values. C.I. is given by (`r round(t_res$conf.int[1], 4)`, `r round(t_res$conf.int[2], 4)`). 
    
    


### 1 - C


Based on some literature review, many psychology and social science data analysis methods refer to this method as 
Cohen's d. All sources I reviewed state that in practice effect sizes between 0.2 and 0.5 are considered as Medium 
size. 

A good explanation is given here: 
https://datatab.net/tutorial/effect-size-independent-t-test

We calculate the effect size as the difference divided by the pooled variance, where pooled variance is given by: 

$$\Large s^2_p = \frac{(n_1 - 1) * s^2_1 + (n_2 - 1) * s^2_2}{n_2 + n_1 - 2}$$
And effect size is given by: 

$$\Large E.S. = \frac{\bar X_2 - \bar X_1}{\sqrt{S^2_p}}$$

```{R}

pooled_variance <- 
  (df_1 * s_1 + df_2 * s_2) / (df_1 + df_2)

effect_size <- 
  difference / sqrt(pooled_variance)

```

I will rely on a test instead because we have more accessible and straightforward way to get a confidence interval, that is 
widely known and accepted among the research community. 

However, effect size is also a good method that gives us an intuitive tool for inference. We took the ratio of a difference 
to the common, or pooled, standard deviation. With our estimate of `r round(effect_size,4)`, we have a moderate effect size. 
Also, difference between samples is approximately `r 100*round(effect_size,2)`% of the common standard deviation. So, the effect is 
, perhaps, "hidden" due to great variance in the sample, or it is "diluted" due to the variance that occurs in the sample. 

Another interpretation is that the difference between the two groups is small when compared to the average, or pooled, variance 
in the sample. There is too much variance to make conclusive statements. 

\newpage 

# Problem 2 

```{r}
p2_data <- read_xlsx("/Users/denisostroushko/Desktop/UofM MS/MS Fall 2022/Puhb 7405/Data Sets/Exam 2/BMT-Trial-Orig.xlsx")

nrow(p2_data)

length(which(is.na(p2_data$duration)))

```


Comment on outliers and describe the data set 

### 2 - A

```{r}
fit <- survfit(Surv(duration, status) ~ Treatment, data = p2_data)
plot <- 
  ggsurvplot(fit, data = p2_data, surv.median.line = "hv") 
  # ggtitle(paste("Kaplan Meier Survival Curves. \n   Cases Median Survival Time: ", case_median_t, 
  #               "days. \n   Controls Median Survival Time: ", cont_median_t, " days.")) 
plot$plot
```

Add model statement here 

```{r}

surv_m1 <- coxph(Surv(duration, status) ~ Treatment, data = p2_data)

res_reg <- data.frame(summary(surv_m1)$coefficients)

res_reg$var <- rownames(res_reg)
rownames(res_reg) <- NULL
res_reg <- res_reg %>% select(var, everything())

res_reg <-
  res_reg %>% mutate_at(vars(coef, exp.coef., se.coef., z, `Pr...z..`),
                                 funs(round(., 6)
                                      )
                                 )

colnames(res_reg) <- c("Predictor", "Estiamte", "Exponentiated Estiamte", "Standard Error", "Z Value", "P value")
res_reg %>%
  kbl(booktabs = T, align = c('l','c', 'c', 'c', 'c', 'c')) %>%
  kable_styling(latex_options = c("striped", "HOLD_position"))
```

Interpret the absolute lack of difference between the two groups 

### 2 - B 

```{r}

surv_m2 <- coxph(Surv(duration, status) ~ 
                    GENDER + age + Treatment + Race  + `HLA-Match`, data = p2_data)

res_reg2 <- data.frame(summary(surv_m2)$coefficients)

res_reg2$var <- rownames(res_reg2)
rownames(res_reg2) <- NULL
res_reg2 <- res_reg2 %>% select(var, everything())

res_reg2 <-
  res_reg2 %>% mutate_at(vars(coef, exp.coef., se.coef., z, `Pr...z..`),
                                 funs(round(., 6)
                                      )
                                 )

colnames(res_reg2) <- c("Predictor", "Estiamte", "Exponentiated Estiamte", "Standard Error", "Z Value", "P value")

res_reg2 %>%
  kbl(booktabs = T, align = c('l','c', 'c', 'c', 'c', 'c')) %>%
  kable_styling(latex_options = c("striped", "HOLD_position"))

```

### 2 - C

Make a table with model estimates from 

```{r}

part_c <- 
  
  cbind(
  
    data.frame(
      Model = c("Full", "Treatment Only")
    ), 
    rbind(
      res_reg2 %>% filter(Predictor == "Treatment"),
      res_reg %>% filter(Predictor == "Treatment")
    )
  )

part_c %>%
  kbl(booktabs = T, align = c('l','c', 'c', 'c', 'c', 'c')) %>%
  kable_styling(latex_options = c("striped", "HOLD_position"))


```
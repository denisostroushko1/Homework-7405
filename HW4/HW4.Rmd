---
title: "Homework 4"
author: "Denis Ostroushko"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r, echo = F}
knitr::opts_chunk$set(echo = T, message = F, warning = F, fig.pos = "!H", out.extra = "")
options(scipen=999)
```

```{r, include=F}
library(tidyverse)
library(kableExtra)
library(readxl)
```

```{r, eval=F}
library(tidyverse)
library(kableExtra)
library(readxl)
```

# 8.4

In this problem we will look at two additive multiple linear models. We will fit the models, and look for variables 
that are statistically associated with the calculated response variables. 

We can fit the model, get the summary and look at p-values that come out from a t-test for each $\hat \beta_i$. 

However, to stay aligned with PUBH 7405 material, we will perform an ANOVA test for the model first, and then 
look at the individual t-test. Additionally, we will implement a Bonferroni, Holm, and Hockberg adjustments. 

```{r, echo  = F}
e_cig_3 <- read_xlsx("/Users/denisostroushko/Desktop/UofM MS/MS Fall 2022/Puhb 7405/Data Sets/E-CID-3.xlsx") 
```

### 8.4 - A

First, let's look at the distribution of the calculated response variable, it is a good practice to do so going 
forward for model development and diagnostics purpose. 

```{r, echo=F}
#response
e_cig_3$Y1 <-log( e_cig_3$NNAL_vt4_creat/e_cig_3$NNAL_vt0_creat )

# fix some variables and their class: 
e_cig_3$income30 <- as.factor(e_cig_3$income30)
e_cig_3$gender <- as.factor(e_cig_3$gender)
e_cig_3$white <- as.factor(e_cig_3$white)
e_cig_3$educ2 <- as.factor(e_cig_3$educ2)
```

```{r}

ggplot(data = e_cig_3,
       aes(x = Y1)) +
  geom_histogram(binwidth = .1, color = "black", fill = "light blue") +
  theme_minimal() +
  ylab("Count") +  
  xlab("Ratio of Final to Baseline NNAL Measurements") + 
  ggtitle("Ratio of NNAL Measurements on the Natural Logarithmic Scale")
```

##### ANOVA Test for all predictors 

We will conduct a One-Way ANOVA test here to see how good the model is at explaining variation in the response variable.

For learning purposes, we will fit the models using built-in functions, and calculate the $F$ statistic by hand. 

In R, we need to fit a model with no predictors, i.e. the one that just predicts/fits the average value of the 
response for all observations in the data set. We then compare a model with more predictors to see if all coefficients 
are equal to zero, or not.

In the code chunk below we obtain the following estimates that we need to calculate $F$ statistic: 

* we obtain $MSR$ and $MSE$ from $SSR$ and $SSE$ respectively. Residuals and Fitted Values come from fitted model using 
  an R function

* degrees on freedom in the numerator is the degrees of freedom of $MSR$, which is the number of predictors minus one 

* degrees of freedom in the denominator is the degrees of freedom of $MSE$, which is the number of observations in the 
  sample minus the number of predictors plus one 
  
```{r}
e_cig_3_model_data <-
  e_cig_3 %>% select(arm, age, gender, white, educ2, income30, FTND, Y1)

model_8.4 <- lm(Y1 ~ ., data = e_cig_3_model_data)

df_msr <- length(e_cig_3_model_data) - 1
df_mse <- nrow(e_cig_3_model_data) - length(e_cig_3_model_data) 

MSR <- sum((mean(e_cig_3_model_data$Y1) - model_8.4$fitted.values)^2)/ # this is SSR: (fitted - mean)^2
            (df_msr) # this is DF = p - 1
          
MSE <- sum((e_cig_3_model_data$Y1 - model_8.4$fitted.values)^2)/ # this is SSE: (fitted - observed)^2
            (df_mse) # this is DF = n - p

F_stat <- MSR/MSE


F_star <- qf(1-.05/2, df1 = df_msr, df2 = df_mse)

P_F_star <- 1 - pf(F_stat, df1 = df_msr, df2 = df_mse)

```

Now we can conduct a test and see if all predictors are 0 or not. Test hypothesis and results are: 

* Null Hypothesis: $H_0: \beta_0 = \beta_1 = ... = \beta_{p-1} = 0$

* Alternative hypothesis: not all $\beta_i$ are 0

* $F$- statistic: `r round(F_stat,2)` 

* Cutoff $F^*$ statistic: `r round(F_star,2)` with $df(MSR) =$ `r df_msr`  and $df(MSE) =$ `r df_mse`

* So, $F < F^*$, therefore, we do not reject the null hypothesis and we do not have enough evidence to conclude that 
  coefficient estimates $\beta_i$ are statistically different from 0. 
  
* Additionally, $P(F^* > F) =$ `r round(P_F_star,3)`, which is kind of close to 0.05. So, when we do individual t-tests
  for each estimate $\beta_i$ we will see that some of those coefficients are somewhat close to being significant, but 
  we do not have enough data or a good enough model fit to detect any evidence that a given predictor is 
  statistically related to the response variable 
 
 
We can also check our work with the built in R function. We need to fit an "empty" model, that predicts/fits an 
average value of $Y$ for each obserbation. Of course, we could also do it by hand. 

```{r}

empty <- lm(Y1 ~ 1, data = e_cig_3_model_data)

anova_res <- data.frame(anova(empty, model_8.4))

anova_res$model <- c("Empty Model", "Extended Model")

anova_res <- anova_res %>% dplyr::select(model, everything())

colnames(anova_res)[1] <- c("Model")

anova_res %>% 
  kbl(booktabs = T, align = 'c', centering = T) %>% 
  kable_styling(latex_options = c("striped", "HOLD_position"))

```

Coefficients and other statistics from the multiple regression model are given in the table below. 

```{r}
model_8.4_res <- summary(model_8.4)
model_8.4_res_df <- data.frame(model_8.4_res$coefficients)
model_8.4_res_df$var <- rownames(model_8.4_res_df)
rownames(model_8.4_res_df) <- NULL
model_8.4_res_df <- model_8.4_res_df %>% select(var, everything())
model_8.4_res_df <-
  model_8.4_res_df %>% mutate_at(vars(Estimate, `Std..Error`, t.value, `Pr...t..`),
                                 funs(round(., 3)
                                      )
                                 )
colnames(model_8.4_res_df) <- c("Predictor", "Estiamte", "Standard Error", "T Value", "P value")

model_8.4_res_df %>%
  kbl(booktabs = T, align = c('l','c', 'c', 'c', 'c')) %>%
  kable_styling(latex_options = c("striped", "HOLD_position"))
```

As we can see, estimated calculated "by hand" align with the built in funcitons. 

##### Summary of all coeffcients 

Comments: 

* None of the variables appear to be statistically significantly related to the response, after adjusting 
  for other variables, at the 5% level. 
  
* However, p-value for the income variable is suggestive that there might be some relationship going on, 
  which we potentially can uncover either with a better model or with more data. Income summary is given below: 
  
```{r}
sum_income <- 
  e_cig_3 %>% 
    group_by(income30) %>% 
    dplyr::summarise(
      n = n(), 
      mean = mean(Y1), 
      median = median(Y1)
    )


sum_income$income30 <- c("<= $30K/Yr.", "> $30K/Yr.")

colnames(sum_income) <- c("Income Levels", "N", "Average Response", "Median Response")

sum_income %>% 
  kbl(align = 'c', booktabs = T) %>% 
  kable_styling(latex_options = 'striped')
```

* While the average response appears to be quite different between the two groups, other variables in the 
  multiple linear model might have an effect on this relationship. 


### 8.4 - B

The distribution of the response variable below is highly skewed, so, perhaps, we should expect an even more 
poor fit of the model, and less statistically significant number of predictors. 

```{r}
#response
e_cig_3$Y2 <-log( e_cig_3$TNE_vt0_creat /e_cig_3$TNE_vt4_creat )

ggplot(data = e_cig_3,
       aes(x = Y2)) +
  geom_histogram(binwidth = .1, color = "black", fill = "light blue") +
  theme_minimal() +
  ylab("Count") +
  xlab("Ratio of Final to Baseline TNE Measurements") +
  ggtitle("Ratio of TNE Measurements on the Natural Logarithmic Scale")

```

```{r}
e_cig_3_model_data <-
  e_cig_3 %>% select(arm, age, gender, white, educ2, income30, FTND, Y2)
model_8.4 <- lm(Y2 ~ ., data = e_cig_3_model_data)
model_8.4_res <- summary(model_8.4)
model_8.4_res_df <- data.frame(model_8.4_res$coefficients)
model_8.4_res_df$var <- rownames(model_8.4_res_df)
rownames(model_8.4_res_df) <- NULL
model_8.4_res_df <- model_8.4_res_df %>% select(var, everything())
model_8.4_res_df <-
  model_8.4_res_df %>% mutate_at(vars(Estimate, `Std..Error`, t.value, `Pr...t..`),
                                 funs(round(., 3)
                                      )
                                 )
colnames(model_8.4_res_df) <- c("Predictor", "Estiamte", "Standard Error", "T Value", "P value")
model_8.4_res_df %>%
  kbl(booktabs = T, align = c('l','c', 'c', 'c', 'c')) %>%
  kable_styling(latex_options = c("striped", "HOLD_position"))

```

* None of the variables here are close to being statistically significant 

* Therefore, none of the predictors help us explain the variance of the biomarker change over time. 

# 9.3

```{r}
data_9.3 <-
data.frame(
  x = c(
    24,
    28,
    32,
    36,
    40,
    44,
    48,
    52,
    56,
    60
    ),
  y = c(
    38.8,
    39.5,
    40.3,
    40.7,
    41.0,
    41.1,
    41.4,
    41.6,
    41.8,
    41.9
    )
  )

data_9.3 %>% kbl() %>% 
  kable_styling(latex_options = c("striped"))

res1 <- t(data_9.3$y) %*% data_9.3$y

res2 <- t(data_9.3$x) %*% data_9.3$y

res3 <- t(data_9.3$x) %*% data_9.3$x

```

* $Y`Y = res1 =$ `r res1`

* $X`Y = res2 =$ `r res2`

* $X`X = res3 =$ `r res3`